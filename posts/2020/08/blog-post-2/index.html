<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Soft Reinforcement Learning: More Details">
<meta property="og:type" content="article">
<meta property="og:title" content="Soft Reinforcement Learning: More Details">
<meta property="og:url" content="http://lanseyege.github.io/posts/2020/08/blog-post-2/index.html">
<meta property="og:site_name" content="Ye Yuan">
<meta property="og:description" content="Soft Reinforcement Learning: More Details">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-08-04T22:00:00.000Z">
<meta property="article:modified_time" content="2021-02-07T19:38:31.027Z">
<meta property="article:author" content="Ye Yuan">
<meta property="article:tag" content="soft">
<meta property="article:tag" content="entropy-regularized">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://lanseyege.github.io/posts/2020/08/blog-post-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Soft Reinforcement Learning: More Details | Ye Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ye Yuan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/posts/2020/08/blog-post-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Soft Reinforcement Learning: More Details
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-05T00:00:00+02:00">2020-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-07 20:38:31" itemprop="dateModified" datetime="2021-02-07T20:38:31+01:00">2021-02-07</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="soft-reinforcement-learning-more-details">Soft Reinforcement Learning: More Details</h1>
<a id="more"></a>
<p>Soft Q-learning learns policies for continuous state and actions. The soft Q-learning meets contraction property, hence, it can converge to the optimal policy. Here, we look into the certify process.</p>
<h2 id="soft-q-iteration">Soft Q-iteration</h2>
<p><strong><em>Theorem: soft Q-iteration.</em></strong> Let <span class="math inline">\(Q_\text{soft}(\cdot, \cdot)\)</span> and <span class="math inline">\(V_\text{soft}(\cdot)\)</span> be bounded and assume that <span class="math inline">\(\int_A \exp (\frac{1}{\alpha}Q_\text{soft}(\cdot, a^\prime))da^\prime &lt; \infty\)</span> and that <span class="math inline">\(Q_\text{soft}^\star &lt; \infty\)</span> exists. Then the fixed-point iteration</p>
<p><span class="math display">\[
\begin{equation}
Q_\text{soft}(s_t, a_t) \leftarrow r_t + \gamma \mathbb{E}_{s_{t+1}\sim p_s}[V_\text{soft}(s_{t+1})], \forall s_t, a_t
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
V_\text{soft}(s_t) \leftarrow \alpha \log \int_A \exp (\frac{1}{\alpha}Q_\text{soft}(s_t, a^\prime))da^\prime, \forall s_t
\end{equation}
\]</span></p>
<p>converges to <span class="math inline">\(Q^\star_\text{soft}\)</span> and <span class="math inline">\(V_\text{soft}^\star\)</span>, respectively.</p>
<h2 id="soft-bellman-operator">“Soft” Bellman Operator</h2>
<p>Soft value iteration operator <span class="math inline">\(\mathcal{T}\)</span> is defined as</p>
<p><span class="math display">\[
\begin{equation}
\mathcal{T} Q(s, a) \triangleq r(s, a) + \gamma \mathbb{E}_{s^\prime \sim p_s}[\log \int \exp Q(s^\prime, a^\prime) da^\prime]
\end{equation}
\]</span></p>
<p>This is a contraction mapping. This result is certified in <a href="#1">[1]</a>, that we can get <span class="math inline">\(\left\|\mathcal{T} Q_{1}-\mathcal{T} Q_{2}\right\| \leq \gamma \varepsilon=\gamma \| Q_{1}-Q_{2} \mid\)</span>. Since the soft Bellman backup is a contraction, the optimal value function is the fixed point of the Bellman backup, and it can be found by optimizing for a Q-function for which the soft Bellman error <span class="math inline">\(\|\mathcal{T}Q-Q\|\)</span> is minimized at all states and actions.</p>
<h2 id="simple-from-the-soft-q-function">Simple from the Soft Q-function</h2>
<p>Soft Q-learning needs to sample from the policy, <span class="math inline">\(\pi(a_t|s_t) \propto \exp (\frac{1}{\alpha}Q_\text{soft}^\theta(s_t, a_t))\)</span>, both to take on-policy actions and to generate action samples for estimating the soft value function. Since the form of the policy is so general, the direct sampling is intractable. The paper adopt an approximate way to do this, let’s see, Stein variational gradient descent (SVGD) <a href="#3">[3]</a>. <strong><em>firstly</em></strong>, they learn a state-conditioned stochastic neural network <span class="math inline">\(a_t = f^\phi(\xi; s_t)\)</span>, parametrized by <span class="math inline">\(\phi\)</span>, that maps noise samples <span class="math inline">\(\xi\)</span> drawn from a distribution into unbiased action samples from the target EBM corresponding to <span class="math inline">\(Q_\text{soft}^\theta\)</span>. (<strong>secondly,</strong>) The induced distribution of the actions are <span class="math inline">\(\pi^\phi(a_t | s_t)\)</span>, the goal is to find parameters <span class="math inline">\(\phi\)</span> so that the induced distribution approximates the energy-based distribution in terms of the KL divergence. <strong>We will list the details of SVGD in another blog.</strong> The paper also indicates the relation of their algos <span class="math inline">\(f^\phi(\xi;s_t)\)</span> and policy-gradient based methods, which you can find more details in <a href="#2">[2]</a> (maybe we will organize these in another blog).</p>
<h1 id="soft-actor-critic-demystified-4">Soft Actor-Critic Demystified (<a href="#4">[4]</a>)</h1>
<p><strong><em>SAC</em></strong> is defined for RL tasks involving continuous actions. The biggest feature of SAC is that it uses a modified RL objective function. Instead of only seeking to maximize the lifetime rewards, SAC seeks to also maximize the entropy of the policy. A high entropy in the policy will explicitly encourage exploration, to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q-function.</p>
<h2 id="sac-architecture">SAC architecture</h2>
<p>The optimization objective:</p>
<p><span class="math display">\[
J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t, a_t)\sim \rho_\pi}[r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))]
\]</span></p>
<p>SAC makes use of three networks:</p>
<ul>
<li>a state value function <span class="math inline">\(V\)</span> parameterized by <span class="math inline">\(\psi\)</span></li>
<li>a soft Q-function <span class="math inline">\(Q\)</span> parameterized by <span class="math inline">\(\theta\)</span></li>
<li>and a policy function <span class="math inline">\(\pi\)</span> parameterized by <span class="math inline">\(\phi\)</span></li>
</ul>
<h3 id="we-train-the-value-network-by-minimizing-the-following-error">We train the value network by minimizing the following error:</h3>
<p><span class="math display">\[
J_V(\psi) = \mathbb{E}_{s_t \sim \mathcal{D}}[\frac{1}{2}(V_{\psi}(s_t) - \mathbb{E}_{a_t \sim \pi_\phi}[Q_\theta (s_t, a_t) - \log \pi_\phi (a_t | s_t)])^2]
\]</span></p>
<p>The approximation of the derivative of the above objective to update the parameters of the V function:</p>
<p><span class="math display">\[
\hat{\nabla}_{\psi}J_V(\psi) = \nabla_\psi V_{\psi}(s_t) (V_{\psi}(s_t) - Q_\theta (s_t, a_t) + \log \pi_{\phi}(a_t | s_t))
\]</span></p>
<h3 id="we-train-the-q-network-by-minimizing-the-following-error">We train the Q network by minimizing the following error:</h3>
<p><span class="math display">\[
J_Q(\theta) = \mathbb{E}_{(s_t, a_t ) \sim \mathcal{D}}\left[ \frac{1}{2}\left(Q_\theta (s_t, a_t) - \hat{Q}(s_t, a_t)\right)^2 \right]
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\hat{Q}(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V_{\bar{\psi}}(s_{t+1})]
\]</span></p>
<p>For all s-a pairs in the experience replay buffer, we want to minimize the squared difference between the prediction of our Q function and the immediate (one time-step) reward plus teh discounted expected Value of the next state. <span class="math inline">\(V_{\bar{\psi}}\)</span> is the target value function.</p>
<p>We use the below approximation of the derivative of the above objective is to update the parameters of the Q function:</p>
<p><span class="math display">\[
\hat{\nabla}_{\theta} J_Q (\theta) = \nabla_{\theta}Q_{\theta}(a_t, s_t) (Q_\theta (s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1}))
\]</span></p>
<h3 id="we-train-the-policy-network-pi-by-minimizing-the-following-error">We train the policy network <span class="math inline">\(\pi\)</span> by minimizing the following error</h3>
<p><span class="math display">\[
J_{\pi}(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[D_{KL}\left(\pi(\cdot | s_t) || \frac{\exp (Q_\theta (s_t, \cdot))}{Z_\theta (s_t)}\right)\right]
\]</span></p>
<p>In order to minimize this objective, the authors use something called the reparameterization trick. This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors. The policy is now parameterized as follows:</p>
<p><span class="math display">\[
a_t = f_{\phi} (\epsilon_t ; s_t)
\]</span></p>
<p>The epsilon term is a noise vector sampled from a Gaussian distribution. Now we can express the objective function as follows:</p>
<p><span class="math display">\[
J_{\pi} (\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\log \pi_{\phi}(f_{\phi}(\epsilon_t ; s_t) | s_t) - Q_{\theta}(s_t , f_{\phi} (\epsilon_t ; s_t))]
\]</span></p>
<p>The normalizing function <span class="math inline">\(Z\)</span> is dropped since it does not depend on the parameter <span class="math inline">\(\phi\)</span>. An unbiased estimator for the gradient of the above objective is given as follows:</p>
<p><span class="math display">\[
\begin{array}{ll}
\hat{\nabla}_{\phi} J_{\pi}  (\phi)  &amp;= \nabla_{\phi} \log \pi_{\phi}(a_t | s_t) \\
&amp; + (\nabla_{a_t} \log \pi_{\phi} (a_t | s_t) - \nabla_{a_t} Q(s_t, a_t))\nabla_{\phi} f_{\phi}(\epsilon_t ; s_t) \\
\end{array}
\]</span></p>
<h1 id="references">References</h1>
<p><a id="1">[1]</a> Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conf. on Uncertainty in Artificial Intelligence, 2016.</p>
<p><a id="2">[2]</a> Schulman, J., Abbeel, P., and Chen, X. Equivalence be-tween policy gradients and soft Q-learning.arXiv preprintarXiv:1704.06440, 2017a.</p>
<p><a id="3">[3]</a> Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016</p>
<p><a id="4">[4]</a> https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/soft/" rel="tag"># soft</a>
              <a href="/tags/entropy-regularized/" rel="tag"># entropy-regularized</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/08/01/2020-08-01-blog-post-1/" rel="prev" title="Soft Reinforcement Learning: A Shallow Overview ">
      <i class="fa fa-chevron-left"></i> Soft Reinforcement Learning: A Shallow Overview 
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/2020/08/blog-post-3/" rel="next" title="程序员数学防忽悠备忘录">
      程序员数学防忽悠备忘录 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#soft-reinforcement-learning-more-details"><span class="nav-number">1.</span> <span class="nav-text">Soft Reinforcement Learning: More Details</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-q-iteration"><span class="nav-number">1.1.</span> <span class="nav-text">Soft Q-iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-bellman-operator"><span class="nav-number">1.2.</span> <span class="nav-text">“Soft” Bellman Operator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-from-the-soft-q-function"><span class="nav-number">1.3.</span> <span class="nav-text">Simple from the Soft Q-function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#soft-actor-critic-demystified-4"><span class="nav-number">2.</span> <span class="nav-text">Soft Actor-Critic Demystified ([4])</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#sac-architecture"><span class="nav-number">2.1.</span> <span class="nav-text">SAC architecture</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#we-train-the-value-network-by-minimizing-the-following-error"><span class="nav-number">2.1.1.</span> <span class="nav-text">We train the value network by minimizing the following error:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#we-train-the-q-network-by-minimizing-the-following-error"><span class="nav-number">2.1.2.</span> <span class="nav-text">We train the Q network by minimizing the following error:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#we-train-the-policy-network-pi-by-minimizing-the-following-error"><span class="nav-number">2.1.3.</span> <span class="nav-text">We train the policy network \(\pi\) by minimizing the following error</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#references"><span class="nav-number">3.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">38</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">84</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lanseyege" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lanseyege" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lanseyege@gmail.com" title="E-Mail → mailto:lanseyege@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
