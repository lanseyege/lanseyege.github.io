<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Soft Reinforcement Learning: More Details">
<meta property="og:type" content="article">
<meta property="og:title" content="Soft Reinforcement Learning: More Details">
<meta property="og:url" content="http://lanseyege.github.io/posts/2020/08/blog-post-2/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Soft Reinforcement Learning: More Details">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-08-04T22:00:00.000Z">
<meta property="article:modified_time" content="2021-02-04T03:00:32.552Z">
<meta property="article:author" content="Ye Yuan">
<meta property="article:tag" content="soft">
<meta property="article:tag" content="entropy-regularized">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://lanseyege.github.io/posts/2020/08/blog-post-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Soft Reinforcement Learning: More Details | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/posts/2020/08/blog-post-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Soft Reinforcement Learning: More Details
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-05 00:00:00" itemprop="dateCreated datePublished" datetime="2020-08-05T00:00:00+02:00">2020-08-05</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-04 04:00:32" itemprop="dateModified" datetime="2021-02-04T04:00:32+01:00">2021-02-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="soft-reinforcement-learning-more-details">Soft Reinforcement Learning: More Details</h1>
<a id="more"></a>
<p>Soft Q-learning learns policies for continuous state and actions. The soft Q-learning meets contraction property, hence, it can converge to the optimal policy. Here, we look into the certify process.</p>
<h2 id="soft-q-iteration">Soft Q-iteration</h2>
<p><strong><em>Theorem: soft Q-iteration.</em></strong> Let <span class="math inline">\(Q_\text{soft}(\cdot, \cdot)\)</span> and <span class="math inline">\(V_\text{soft}(\cdot)\)</span> be bounded and assume that <span class="math inline">\(\int_A \exp (\frac{1}{\alpha}Q_\text{soft}(\cdot, a^\prime))da^\prime &lt; \infty\)</span> and that <span class="math inline">\(Q_\text{soft}^\star &lt; \infty\)</span> exists. Then the fixed-point iteration</p>
<p><span class="math display">\[
\begin{equation}
Q_\text{soft}(s_t, a_t) \leftarrow r_t + \gamma \mathbb{E}_{s_{t+1}\sim p_s}[V_\text{soft}(s_{t+1})], \forall s_t, a_t
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}
V_\text{soft}(s_t) \leftarrow \alpha \log \int_A \exp (\frac{1}{\alpha}Q_\text{soft}(s_t, a^\prime))da^\prime, \forall s_t
\end{equation}
\]</span></p>
<p>converges to <span class="math inline">\(Q^\star_\text{soft}\)</span> and <span class="math inline">\(V_\text{soft}^\star\)</span>, respectively.</p>
<h2 id="soft-bellman-operator">“Soft” Bellman Operator</h2>
<p>Soft value iteration operator <span class="math inline">\(\mathcal{T}\)</span> is defined as</p>
<p><span class="math display">\[
\begin{equation}
\mathcal{T} Q(s, a) \triangleq r(s, a) + \gamma \mathbb{E}_{s^\prime \sim p_s}[\log \int \exp Q(s^\prime, a^\prime) da^\prime]
\end{equation}
\]</span></p>
<p>This is a contraction mapping. This result is certified in <a href="#1">[1]</a>, that we can get <span class="math inline">\(\left\|\mathcal{T} Q_{1}-\mathcal{T} Q_{2}\right\| \leq \gamma \varepsilon=\gamma \| Q_{1}-Q_{2} \mid\)</span>. Since the soft Bellman backup is a contraction, the optimal value function is the fixed point of the Bellman backup, and it can be found by optimizing for a Q-function for which the soft Bellman error <span class="math inline">\(\|\mathcal{T}Q-Q\|\)</span> is minimized at all states and actions.</p>
<h2 id="simple-from-the-soft-q-function">Simple from the Soft Q-function</h2>
<p>Soft Q-learning needs to sample from the policy, <span class="math inline">\(\pi(a_t|s_t) \propto \exp (\frac{1}{\alpha}Q_\text{soft}^\theta(s_t, a_t))\)</span>, both to take on-policy actions and to generate action samples for estimating the soft value function. Since the form of the policy is so general, the direct sampling is intractable. The paper adopt an approximate way to do this, let’s see, Stein variational gradient descent (SVGD) <a href="#3">[3]</a>. <strong><em>firstly</em></strong>, they learn a state-conditioned stochastic neural network <span class="math inline">\(a_t = f^\phi(\xi; s_t)\)</span>, parametrized by <span class="math inline">\(\phi\)</span>, that maps noise samples <span class="math inline">\(\xi\)</span> drawn from a distribution into unbiased action samples from the target EBM corresponding to <span class="math inline">\(Q_\text{soft}^\theta\)</span>. (<strong>secondly,</strong>) The induced distribution of the actions are <span class="math inline">\(\pi^\phi(a_t | s_t)\)</span>, the goal is to find parameters <span class="math inline">\(\phi\)</span> so that the induced distribution approximates the energy-based distribution in terms of the KL divergence. <strong>We will list the details of SVGD in another blog.</strong> The paper also indicates the relation of their algos <span class="math inline">\(f^\phi(\xi;s_t)\)</span> and policy-gradient based methods, which you can find more details in <a href="#2">[2]</a> (maybe we will organize these in another blog).</p>
<h2 id="references">References</h2>
<p><a id="1">[1]</a> Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conf. on Uncertainty in Artificial Intelligence, 2016.</p>
<p><a id="2">[2]</a> Schulman, J., Abbeel, P., and Chen, X. Equivalence be-tween policy gradients and soft Q-learning.arXiv preprintarXiv:1704.06440, 2017a.</p>
<p><a id="3">[3]</a> Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/soft/" rel="tag"># soft</a>
              <a href="/tags/entropy-regularized/" rel="tag"># entropy-regularized</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/08/01/2020-08-01-blog-post-1/" rel="prev" title="Soft Reinforcement Learning: A Shallow Overview ">
      <i class="fa fa-chevron-left"></i> Soft Reinforcement Learning: A Shallow Overview 
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#soft-reinforcement-learning-more-details"><span class="nav-number">1.</span> <span class="nav-text">Soft Reinforcement Learning: More Details</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-q-iteration"><span class="nav-number">1.1.</span> <span class="nav-text">Soft Q-iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#soft-bellman-operator"><span class="nav-number">1.2.</span> <span class="nav-text">“Soft” Bellman Operator</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#simple-from-the-soft-q-function"><span class="nav-number">1.3.</span> <span class="nav-text">Simple from the Soft Q-function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#references"><span class="nav-number">1.4.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
