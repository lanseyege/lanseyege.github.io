<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Bayesian Inference">
<meta property="og:type" content="article">
<meta property="og:title" content="Bayesian Inference">
<meta property="og:url" content="http://lanseyege.github.io/posts/2021/02/blog-post-37/index.html">
<meta property="og:site_name" content="Ye Yuan">
<meta property="og:description" content="Bayesian Inference">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://lanseyege.github.io/images/bi.png">
<meta property="og:image" content="http://lanseyege.github.io/images/bi2.png">
<meta property="article:published_time" content="2021-02-07T23:00:00.000Z">
<meta property="article:modified_time" content="2021-02-18T09:27:14.114Z">
<meta property="article:author" content="Ye Yuan">
<meta property="article:tag" content="Bayesian">
<meta property="article:tag" content="Inference">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://lanseyege.github.io/images/bi.png">

<link rel="canonical" href="http://lanseyege.github.io/posts/2021/02/blog-post-37/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Bayesian Inference | Ye Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ye Yuan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/posts/2021/02/blog-post-37/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Bayesian Inference
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-08 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-08T00:00:00+01:00">2021-02-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-18 10:27:14" itemprop="dateModified" datetime="2021-02-18T10:27:14+01:00">2021-02-18</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="bayesian-inference">Bayesian Inference</h1>
<a id="more"></a>
<h2 id="what-is-bayesian-inference">What is Bayesian Inference?</h2>
<p><strong><em>Statistical inference</em></strong> is the process of deducing properties about a population or probability distribution from data. <strong><em>Bayesian inference</em></strong> is therefore just the process of deducing properties about a population or probability distribution from data using Bayes' therem. <a href="#1">[1]</a></p>
<p>Bayesian inference is a method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available. <a href="#2">[2]</a></p>
<p><strong><em>Statistical inference</em></strong> consists in learning about what we do not observe based on what we observe. <strong><em>Bayesian inference</em></strong> is the process of producing statistical inference taking a Bayesian point of view. <a href="#3">[3]</a></p>
<figure>
<img src="/images/bi.png" alt="Bayesian Inference" /><figcaption aria-hidden="true">Bayesian Inference</figcaption>
</figure>
<figure>
<img src="/images/bi2.png" alt="Bayesian Inference" /><figcaption aria-hidden="true">Bayesian Inference</figcaption>
</figure>
<h2 id="formal-description-of-bayesian-inference-2">Formal Description of Bayesian Inference <a href="#2">[2]</a></h2>
<p><strong><em>Definitions</em></strong></p>
<ul>
<li><span class="math inline">\(x\)</span>, a data point in general. This may in fact be a vector of values.</li>
<li><span class="math inline">\(\theta\)</span>, the parameter of the data point’s distribution, i.e., <span class="math inline">\(x \sim p(x | \theta)\)</span>. This may in fact be a vector of parameters.</li>
<li><span class="math inline">\(\alpha\)</span>, the hyperparameter of the parameter distribution, i.e., <span class="math inline">\(\theta \sim p(\theta | \alpha)\)</span>. This may in fact be a vector of hyperparameters.</li>
<li><span class="math inline">\(\mathbf{X}\)</span> is the sample, a set of <span class="math inline">\(n\)</span> observed data points, i.e., <span class="math inline">\(x_1, \ldots, x_n\)</span>.</li>
<li><span class="math inline">\(\tilde{x}\)</span>, a new data point whose distribution is to be predicted.</li>
</ul>
<p><strong><em>Bayesian Inference</em></strong></p>
<ul>
<li>The <strong>prior distribution</strong> is the distribution of the parameters before any data is observed, i.e. <span class="math inline">\(p(\theta | \alpha)\)</span>. The prior distribution might not be easily determined; in such a case, one possibility may be to use the <strong>Jeffreys prior</strong> to obtain a prior distribution before updating it with newer observations.</li>
<li>The <strong>sampling distribution</strong> is the distribution of the observed data conditional on its parameters, i.e. <span class="math inline">\(p(\mathbf{X} | \theta)\)</span>. This is also termed the <strong>likelihood</strong>, especially when viewed as a function of the parameters, sometimes written <span class="math inline">\(L(\theta | \mathbf{X}) = p(\mathbf{X} | \theta)\)</span>.</li>
<li>The <strong>marginal likelihood</strong> (sometimes also termed the <strong>evidence</strong>) is the distribution of the observed data marginalized over the parameters, i.e. <span class="math inline">\(p(\mathbf{X} | \alpha) = \int p(\mathbf{X}|\theta) p(\theta | \alpha) d\theta\)</span>.</li>
<li>The <strong>posterior distribution</strong> is the distribution of the parameters after taking into account the observed data. This is determined by Bayes' rule, which forms the heart of Bayesian inference:</li>
</ul>
<p><span class="math display">\[
  \begin{array}{ll}
  p(\theta|\mathbf{X}, \alpha)&amp;=\frac{p(\theta, \mathbf{X}, \alpha)}{p(\mathbf{X}, \alpha)} = \frac{p(\mathbf{X}|\theta, \alpha)p(\theta,\alpha)}{p(\mathbf{X}|\alpha)p(\alpha)} \\
  &amp;=\frac{p(\mathbf{X}|\theta, \alpha)p(\theta|\alpha)}{p(\mathbf{X}|\alpha)} \propto p(\mathbf{X}|\theta, \alpha)p(\theta | \alpha)
  \end{array}
  \]</span></p>
<p><strong><em>Bayesian prediction</em></strong></p>
<ul>
<li>The <strong><em>posterior predictive distribution</em></strong> is the distribution of a new data point, marginalized over the posterior:</li>
</ul>
<p><span class="math display">\[
  p(\tilde{x} | \mathbf{X}, \alpha) = \int p(\tilde{x}|\theta) p(\theta | \mathbf{X}, \alpha) d\theta
  \]</span></p>
<ul>
<li>The <strong>proir predictive distribution</strong> is the distribution of a new data point, marginalized over the prior:</li>
</ul>
<p><span class="math display">\[
  p(\tilde{x}|\alpha) = \int p(\tilde{x}|\theta)p(\theta|\alpha)d\theta
  \]</span></p>
<h3 id="computational-difficulties">Computational difficulties</h3>
<p>The Bayes theorem tells us that the computation of the posterior requires three terms: a prior, a likelihood and evidence. The first two can be expressed easily as they are part of the assumed model. However, the third term, that is normalisation factor, requires to be computed <span class="math inline">\(p(x)=\int_\theta p(x|\theta)p(\theta)d\theta\)</span> (<span class="math inline">\(\alpha\)</span> not shown here).</p>
<h2 id="methods">Methods</h2>
<p>The integral can become <strong>intractable in higher dimensions</strong>. Two most used approached are <strong>Markov Chain Monte Carlo</strong> ( simpling-based) and <strong>Variational Inference</strong> methods (from optimization perspectiver).</p>
<h3 id="mcmc">MCMC</h3>
<p>In MCMC, we <strong>first</strong> construct an ergodic Markov chain on <span class="math inline">\(\theta\)</span> whose stationary distribution is the posterior <span class="math inline">\(p(\theta | x)\)</span>. <strong>Then</strong> we sample from the chain to collect samples from the stationary distribution. <strong>Finally</strong>, we approximate the posterior with an empirical estimate constructed from (a subset of) the collected samples.<a href="#5">[5]</a></p>
<p>Monte carlo sampling methods are able to draw independent samples from distribution. While, they are not effective and may be intractable for high-dimensinal probabilistic models. Markov Chain Monte Carlo provides an alternate approach to random sampling a high-dimensional probability distribution where the next sample is dependent upon the current sample.</p>
<p>Markov chain is a systematic method for generating a sequence of random variables where the current value is probabilistically dependent on the value of the prior variable.</p>
<p>A Markov chain is a special type of stochastic process, which deals with characterization of sequences of random variables. Special interest is paid to the dynamic and the limiting behaviors of the sequence.</p>
<p>MCMC algorithms are sensitive to their starting point, and often require a warm-up phase or burn-in phase to move in towards a fruitful part of the search space, after which prior samples can be discarded and useful samples can be collected.</p>
<h4 id="gibbs-sampling">Gibbs Sampling</h4>
<p>Gibbs Sampling is appropriate for those probabilistic models where this conditional probability can be calculated.</p>
<h4 id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</h4>
<p>The Metropolis-Hastings Algorithm is appropriate for those probabilistic models where we cannot directly sample the so-called next state probability distribution, . Unlike the Gibbs chain, the algorithm doesnot assume that we can generate next state samples from a particular target distribution. Instead, the Metropolis-Hastings algorithm involves using a surrogate or proposal probability distribution that is sampled (sometimes called the kernel), then an acceptance criterion that decides whether the new sample is accepted into the chain or discarded.</p>
<h3 id="varational-inference">Varational Inference</h3>
<p>Variational inference is widely used to approximate posterior densities for Bayesian models, an alternative strategy to MCMC sampling. Compared to MCMC, variational inference tends to be faster and easier to scale to large data.</p>
<h1 id="references">References</h1>
<p><a id="1">[1]</a> https://towardsdatascience.com/probability-concepts-explained-bayesian-inference-for-parameter-estimation-90e8930e5348</p>
<p><a id="2">[2]</a> https://en.wikipedia.org/wiki/Bayesian_inference</p>
<p><a id="3">[3]</a> https://towardsdatascience.com/bayesian-inference-problem-mcmc-and-variational-inference-25a8aa9bce29</p>
<p><a id="4">[4]</a> <span class="citation" data-cites="article">@article</span>{speagle2019conceptual, title={A conceptual introduction to markov chain monte carlo methods}, author={Speagle, Joshua S}, journal={arXiv preprint arXiv:1909.12313}, year={2019} }</p>
<p><a id="5">[5]</a> <span class="citation" data-cites="article">@article</span>{blei2017variational, title={Variational inference: A review for statisticians}, author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D}, journal={Journal of the American statistical Association}, volume={112}, number={518}, pages={859–877}, year={2017}, publisher={Taylor &amp; Francis} }</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Bayesian/" rel="tag"># Bayesian</a>
              <a href="/tags/Inference/" rel="tag"># Inference</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/2021/02/blog-post-36/" rel="prev" title="Anomaly Detection with Imitation Learning and Inverse Reinforcement Learning">
      <i class="fa fa-chevron-left"></i> Anomaly Detection with Imitation Learning and Inverse Reinforcement Learning
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/2021/02/blog-post-38/" rel="next" title=" Notes on book "Simulation (Fifth Edition)" ">
       Notes on book "Simulation (Fifth Edition)"  <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#bayesian-inference"><span class="nav-number">1.</span> <span class="nav-text">Bayesian Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#what-is-bayesian-inference"><span class="nav-number">1.1.</span> <span class="nav-text">What is Bayesian Inference?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#formal-description-of-bayesian-inference-2"><span class="nav-number">1.2.</span> <span class="nav-text">Formal Description of Bayesian Inference [2]</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#computational-difficulties"><span class="nav-number">1.2.1.</span> <span class="nav-text">Computational difficulties</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#methods"><span class="nav-number">1.3.</span> <span class="nav-text">Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mcmc"><span class="nav-number">1.3.1.</span> <span class="nav-text">MCMC</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#gibbs-sampling"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Gibbs Sampling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#metropolis-hastings-algorithm"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Metropolis-Hastings Algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#varational-inference"><span class="nav-number">1.3.2.</span> <span class="nav-text">Varational Inference</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#references"><span class="nav-number">2.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">40</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">88</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lanseyege" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lanseyege" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lanseyege@gmail.com" title="E-Mail → mailto:lanseyege@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
