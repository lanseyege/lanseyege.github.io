<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ReduNet [1]">
<meta property="og:type" content="article">
<meta property="og:title" content="notes on &quot;ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction&quot;">
<meta property="og:url" content="http://lanseyege.github.io/posts/2021/05/blog-post-47/index.html">
<meta property="og:site_name" content="Ye Yuan">
<meta property="og:description" content="ReduNet [1]">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-05-27T07:30:58.371Z">
<meta property="article:modified_time" content="2021-05-31T11:44:37.093Z">
<meta property="article:author" content="Ye Yuan">
<meta property="article:tag" content="rate reduction">
<meta property="article:tag" content="deep learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://lanseyege.github.io/posts/2021/05/blog-post-47/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>notes on "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction" | Ye Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ye Yuan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/posts/2021/05/blog-post-47/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          notes on "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction"
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-05-27 15:30:58" itemprop="dateCreated datePublished" datetime="2021-05-27T15:30:58+08:00">2021-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-31 19:44:37" itemprop="dateModified" datetime="2021-05-31T19:44:37+08:00">2021-05-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="redunet-1">ReduNet <a href="#1">[1]</a></h1>
<a id="more"></a>
<p>This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation.</p>
<p><strong>Motivation</strong> The design of deep networks are often based on years of trial and error, then trained via back propagation, and then deployed as a “black box”. It lacks of rigorous mathematical principles, modeling and analysis. It naturally raises a fundamental question that we aim to address in this paper: <strong><em>how to develop a principled mathematical framework for better understanding and design of deep networks?</em></strong></p>
<p><strong>A new theoretical framework</strong> The paper develop a new theoretical framework for understanding deep networks around the following two questions:</p>
<ul>
<li>Objective of Representation Learning : What intrinsic structures of the data should we learn, and how should we represent such structures? What is a principled objective function for learning a good representation of such structures, instead of choosing heuristically or arbitrarily?</li>
<li>Architecture of Deep Networks : Can we justify the structures of modern deep networks from such a principle? In particular, can the networks’ layered architecture and operators (linear or nonlinear) all be derived from this objective, rather than designed heuristically and evaluated empirically?</li>
</ul>
<p>The paper’s answer to the two questions are:</p>
<ul>
<li>A principled objective for a deep network is to learn a low-dimensional linear discriminative representation of the data. The optimality of such a representation can be evaluated by a principled measure from (lossy) data compression, known as <strong><em>rate reduction</em></strong>.</li>
<li>Deep networks can be naturally interpreted as <strong><em>optimization schemes for maximizing this measure</em></strong>.</li>
</ul>
<p>Not only does this framework offer new perspectives to understand and interpret modern deep networks, they also provide new insights that can potentially change and improve the practice of deep networks.</p>
<h2 id="the-principle-of-maximal-coding-rate-reduction">The Principle of Maximal Coding Rate Reduction</h2>
<p>Whether the given data <span class="math inline">\(\mathbf{X}\)</span> of a mixed distribution <span class="math inline">\(\mathcal{D} = \{\mathcal{D}^j\}_{j=1}^k\)</span> can be effectively classified depends on how separable (or discriminative) the component distribution <span class="math inline">\(\mathcal{D}^j\)</span> are. One popular working <strong><em>assumption</em></strong> is that the distribution of each class has relatively low-dimensional intrinsic structures.</p>
<p>We <strong><em>assume</em></strong> the distribution <span class="math inline">\(\mathcal{D}^j\)</span> of each class has a support on a low-dimensional submanifold, say <span class="math inline">\(\mathcal{M}^j\)</span> with dimension <span class="math inline">\(d_j \ll D\)</span>, and the distribution <span class="math inline">\(\mathcal{D}\)</span> of <span class="math inline">\(x\)</span> is supported on the mixture of those submanifolds, <span class="math inline">\(\mathcal{M} = \cup_{j=1}^k \mathcal{M}^j\)</span>, in the high-dimensional ambient space <span class="math inline">\(\mathbb{R}^D\)</span>.</p>
<p>With the manifold assumption, we want to learn a mapping <span class="math inline">\(z = f(x, \theta)\)</span> that maps each of the submanifolds <span class="math inline">\(\mathcal{M}^j \subset \mathbb{R}^D\)</span> to a linear subspace <span class="math inline">\(\mathbf{S}^j \subset \mathbb{R}^n\)</span>. We require the learned representation to have the following properties, called a linear discriminative representation (LDR):</p>
<ul>
<li>Within-Class Compressible</li>
<li>Between-Class Discriminative</li>
<li>Diverse Representation</li>
</ul>
<p>In this work, to learn a discriminative linear representation for intrinsic low-dimensional structures from high-dimensional data, they propose an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class, known as <strong><em>rate reduction</em></strong>.</p>
<h3 id="measure-of-compactness-for-linear-representation">Measure of Compactness for Linear Representation</h3>
<p><strong><em>Rate distortion</em></strong> measures the “compactness” of a random distribution: Given a random variable <span class="math inline">\(z\)</span> and a prescribed precision <span class="math inline">\(\epsilon &gt; 0\)</span>, the rate distortion <span class="math inline">\(R(z, \epsilon)\)</span> is the minimal number of binary bits needed to encode <span class="math inline">\(z\)</span> such that the expected decoding error is less than <span class="math inline">\(\epsilon\)</span>, i.e., the decoded <span class="math inline">\(\hat{z}\)</span> satisfied <span class="math inline">\(\mathbb{E}[\| z - \hat{z} \| _2] \leq \epsilon\)</span>.</p>
<p><strong><em>Rate distortion for finite samples on a subspace.</em></strong> The compactness of learned features as a whole can be measured in terms of the average coding length per sample (as the sample size m is large), a.k.a. the coding rate subject to the distortion <span class="math inline">\(\epsilon\)</span>:</p>
<p><span class="math display">\[
\begin{equation}
R(\mathbf{Z}, \epsilon) = \frac{1}{2} \log \det (\mathbf{I} + \frac{n}{m \epsilon^2} \mathbf{Z}\mathbf{Z}^\star)
\end{equation}
\]</span></p>
<p><strong><em>Rate distortion of samples on a mixture of subspaces.</em></strong> We may partition the data (representation) <span class="math inline">\(\mathbf{Z}\)</span> into multiple subsets: <span class="math inline">\(\mathbf{Z} = \mathbf{Z}^1 \cup \mathbf{Z}^2 \ldots \mathbf{Z}^k\)</span> with each <span class="math inline">\(\mathbf{Z}^j\)</span> containing samples in one low-dimensional subspace. Let <span class="math inline">\(\Pi = \{ \Pi ^ j \in \mathbb{R}^{m \times m} \}_{j=1}^k\)</span> be a set of diagonal matrices whose diagonal entries encode the membership of the <span class="math inline">\(m\)</span> samples in the <span class="math inline">\(k\)</span> classes. The average number of bits per sample (the coding rate) is</p>
<p><span class="math display">\[
\begin{equation}
R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi}) = \sum_{j = 1} ^k \frac{tr(\mathbf{\Pi})}{2m} \log \det \left( \mathbf{I} + \frac{d}{tr(\mathbf{\Pi}^j)\epsilon ^2} \mathbf{Z}\mathbf{\Pi}^j \mathbf{Z}^\star  \right)
\end{equation}
\]</span></p>
<p>When <span class="math inline">\(\mathbf{Z}\)</span> is given, <span class="math inline">\(R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi})\)</span> is a concave function of <span class="math inline">\(\mathbf{\Pi}\)</span>.</p>
<h3 id="principle-of-maximal-coding-rate-reduction">Principle of Maximal Coding Rate Reduction</h3>
<p>The learned features should follow the basic rule that <strong><em>similarity contracts and dissimilarity contrasts</em></strong>. To be more precise, a good (linear) discriminative representation <span class="math inline">\(\mathbf{Z}\)</span> of <span class="math inline">\(\mathbf{X}\)</span> is one such that, given a partition <span class="math inline">\(\mathbf{\Pi}\)</span> of <span class="math inline">\(\mathbf{Z}\)</span>, achieves a large difference between the coding rate for the whole and that for all the subsets:</p>
<p><span class="math display">\[
\begin{equation}
\Delta R(\mathbf{Z}, \mathbf{\Pi}, \epsilon) = R(\mathbf{Z}, \epsilon) - R_c(\mathbf{Z}, \epsilon | \mathbf{\Pi})
\end{equation}
\]</span></p>
<p>If we choose our feature mapping to be <span class="math inline">\(z = f(x, \theta)\)</span> (a dnn), the overall process of the feature representation and the resulting rate reduction w.r.t. certain partition <span class="math inline">\(\mathbf{\Pi}\)</span> can be illustrated by the following diagram:</p>
<p><span class="math display">\[
\begin{equation}
\boldsymbol{X} \stackrel{f(\boldsymbol{x}, \boldsymbol{\theta})}{\longrightarrow} \boldsymbol{Z}(\boldsymbol{\theta}) \stackrel{\Pi, \epsilon}{\longrightarrow} \Delta R(\boldsymbol{Z}(\boldsymbol{\theta}), \boldsymbol{\Pi}, \epsilon)
\end{equation}
\]</span></p>
<p><strong>Normalization</strong>. To make the amount of reduction comparable between different representations, we need to normalize the scale of the learned features, either by imposing the Frobenius norm of each class <span class="math inline">\(\mathbf{Z}^j\)</span> to scale with the number of features in <span class="math inline">\(\mathbf{Z}^j \in \mathbb{R}^{n \times m_j} : \| \mathbf{Z}^j \| _F^2 = m_j\)</span> or by normalizing each feature to be on the unit sphere: <span class="math inline">\(z ^i \in \mathcal{S}^{n-1}\)</span>.</p>
<p>Once the representations can be compared fairly, our goal becomes to learn a set of features <span class="math inline">\(\mathbf{Z}(\theta) = f(\mathbf{X}, \theta)\)</span> and their partition <span class="math inline">\(\mathbf{\Pi}\)</span> such that they maximize the reduction between the coding rate of all features and that of the sum of features w.r.t. their classes:</p>
<p><span class="math display">\[
\begin{equation}
\begin{split}
\max_{\theta, \Pi} \Delta R(\mathbf{Z}(\mathbf{\theta}), \mathbf{\Pi}, \epsilon) &amp;= R(\mathbf{Z}(\mathbf{\theta}), \epsilon) - R_c (\mathbf{Z}(\theta), \epsilon | \mathbf{\Pi}),  \\
 \text{ s.t. } &amp; \| \mathbf{Z}^j (\mathbf{\theta}) \|_F ^2 = m_j , \mathbf{\Pi} \in \Omega
\end{split}
\end{equation}
\]</span></p>
<p>This is refered to as the principle of maximal coding rate reduction (MC<span class="math inline">\(R^2\)</span>).</p>
<h3 id="properties-of-the-rate-reduction-function">Properties of the Rate Reduction Function</h3>
<p>The MC<span class="math inline">\(R^2\)</span> principle is very general and can be applied to representations <span class="math inline">\(\mathbf{Z}\)</span> of any distributions with any attributes <span class="math inline">\(\mathbf{\Pi}\)</span> as long as the rates <span class="math inline">\(R\)</span> and <span class="math inline">\(R_c\)</span> for the distributions can be accurately and efficiently evaluated.</p>
<p>The optimal representation <span class="math inline">\(\mathbf{Z_\star}\)</span> has the following desired properties:</p>
<p><strong>Theorem 1</strong> (Informal Statement) Suppose <span class="math inline">\(\mathbf{Z_\star} = \mathbf{Z_\star^1} \cup \cdots \cup \mathbf{Z_\star^k}\)</span> is the optimal solution that maximizes the rate reduction with the rates <span class="math inline">\(R\)</span> and <span class="math inline">\(R_c\)</span>. Assume that the optimal solution satisfies <span class="math inline">\(rank(\mathbf{Z_\star^j}) \leq d_j\)</span>. We have:</p>
<ul>
<li>Between-class Discriminative: As long as the ambient space is adequately large <span class="math inline">\((n \geq \sum_{j=1}^k d_j)\)</span>, the subspaces are all orthogonal to each other, i.e. <span class="math inline">\((\mathbf{Z_\star^i}) * \mathbf{Z_\star^j} = 0\)</span> for <span class="math inline">\(i \neq j.\)</span></li>
<li>Maximally Diverse Representation: As long as the coding precision is adequately high, i.e., <span class="math inline">\(\epsilon^4 &lt; \min_j \{\frac{m_j}{m} \frac{n^2}{d_j^2} \}\)</span>, each subspace achieves its maximal dimension, i.e. <span class="math inline">\(rank(\mathbf{Z_\star^j}) = d_j\)</span>. In addition, the largest <span class="math inline">\(d_j - 1\)</span> singular values of <span class="math inline">\(\mathbf{Z_\star}^j\)</span> are equal.</li>
</ul>
<h1 id="references">References</h1>
<p><a id="1">[1]</a> <span class="citation" data-cites="article">@article</span>{chan2020redunet, title={ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction}, author={Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi, Haozhi and Wright, John and Ma, Yi}, journal={arXiv preprint arXiv:2105.10446}, year={2021} }</p>
<p><a id="2">[2]</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/rate-reduction/" rel="tag"># rate reduction</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/2021/05/blog-post-46/" rel="prev" title="Determinantal Point Processes">
      <i class="fa fa-chevron-left"></i> Determinantal Point Processes
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/2021/06/blog-post-49/" rel="next" title="Notes on "Greedy Function Approximation: a Gradient Boosting Machine" ">
      Notes on "Greedy Function Approximation: a Gradient Boosting Machine"  <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#redunet-1"><span class="nav-number">1.</span> <span class="nav-text">ReduNet [1]</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#the-principle-of-maximal-coding-rate-reduction"><span class="nav-number">1.1.</span> <span class="nav-text">The Principle of Maximal Coding Rate Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#measure-of-compactness-for-linear-representation"><span class="nav-number">1.1.1.</span> <span class="nav-text">Measure of Compactness for Linear Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#principle-of-maximal-coding-rate-reduction"><span class="nav-number">1.1.2.</span> <span class="nav-text">Principle of Maximal Coding Rate Reduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#properties-of-the-rate-reduction-function"><span class="nav-number">1.1.3.</span> <span class="nav-text">Properties of the Rate Reduction Function</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#references"><span class="nav-number">2.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">49</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">108</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/lanseyege" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;lanseyege" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:lanseyege@gmail.com" title="E-Mail → mailto:lanseyege@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
