<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="ReduNet [1]">
<meta property="og:type" content="article">
<meta property="og:title" content="notes on &quot;ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction&quot;">
<meta property="og:url" content="http://lanseyege.github.io/posts/2021/05/blog-post-47/index.html">
<meta property="og:site_name" content="Ye Yuan">
<meta property="og:description" content="ReduNet [1]">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2023-01-19T04:35:49.472Z">
<meta property="article:modified_time" content="2021-05-31T11:44:37.093Z">
<meta property="article:author" content="Ye Yuan">
<meta property="article:tag" content="rate reduction">
<meta property="article:tag" content="deep learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://lanseyege.github.io/posts/2021/05/blog-post-47/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>notes on "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction" | Ye Yuan</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ye Yuan</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/posts/2021/05/blog-post-47/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye Yuan">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          notes on "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction"
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2023-01-19 12:35:49" itemprop="dateCreated datePublished" datetime="2023-01-19T12:35:49+08:00">2023-01-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-05-31 19:44:37" itemprop="dateModified" datetime="2021-05-31T19:44:37+08:00">2021-05-31</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="ReduNet-1"><a href="#ReduNet-1" class="headerlink" title="ReduNet [1]"></a>ReduNet <a href="#1">[1]</a></h1><span id="more"></span>

<p>This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. </p>
<p><strong>Motivation</strong> The design of deep networks are often based on years of trial and error, then trained via back propagation, and then deployed as a “black box”. It lacks of rigorous mathematical principles, modeling and analysis. It naturally raises a fundamental question that we aim to address in this paper: <em><strong>how to develop a principled mathematical framework for better understanding and design of deep networks?</strong></em></p>
<p><strong>A new theoretical framework</strong>  The paper develop a new theoretical framework for understanding deep networks around the following two questions:</p>
<ul>
<li>Objective of Representation Learning : What intrinsic structures of the data should we learn, and how should we represent such structures? What is a principled objective function for learning a good representation of such structures, instead of choosing heuristically or arbitrarily?</li>
<li>Architecture of Deep Networks : Can we justify the structures of modern deep networks from such a principle? In particular, can the networks’ layered architecture and operators (linear or nonlinear) all be derived from this objective, rather than designed heuristically and evaluated empirically?</li>
</ul>
<p>The paper’s answer to the two questions are:</p>
<ul>
<li>A principled objective for a deep network is to learn a low-dimensional linear discriminative representation of the data. The optimality of such a representation can be evaluated by a principled measure from (lossy) data compression, known as <em><strong>rate reduction</strong></em>. </li>
<li>Deep networks can be naturally interpreted as <em><strong>optimization schemes for maximizing this measure</strong></em>.</li>
</ul>
<p>Not only does this framework offer new perspectives to understand and interpret modern deep networks, they also provide new insights that can potentially change and improve the practice of deep networks. </p>
<h2 id="The-Principle-of-Maximal-Coding-Rate-Reduction"><a href="#The-Principle-of-Maximal-Coding-Rate-Reduction" class="headerlink" title="The Principle of Maximal Coding Rate Reduction"></a>The Principle of Maximal Coding Rate Reduction</h2><p>Whether the given data $\mathbf{X}$ of a mixed distribution $\mathcal{D} &#x3D; {\mathcal{D}^j}_{j&#x3D;1}^k$ can be effectively classified depends on how separable (or discriminative) the component distribution $\mathcal{D}^j$ are. One popular working <em><strong>assumption</strong></em> is that the distribution of each class has relatively low-dimensional intrinsic structures. </p>
<p>We <em><strong>assume</strong></em> the distribution $\mathcal{D}^j$ of each class has a support on a low-dimensional submanifold, say $\mathcal{M}^j$ with dimension $d_j \ll D$, and the distribution $\mathcal{D}$ of $x$ is supported on the mixture of those submanifolds, $\mathcal{M} &#x3D; \cup_{j&#x3D;1}^k \mathcal{M}^j$, in the high-dimensional ambient space $\mathbb{R}^D$. </p>
<p>With the manifold assumption, we want to learn a mapping $z &#x3D; f(x, \theta)$ that maps each of the submanifolds $\mathcal{M}^j \subset \mathbb{R}^D$ to a linear subspace $\mathbf{S}^j \subset \mathbb{R}^n$. We require the learned representation to have the following properties, called a linear discriminative representation (LDR):</p>
<ul>
<li>Within-Class Compressible</li>
<li>Between-Class Discriminative</li>
<li>Diverse Representation</li>
</ul>
<p>In this work, to learn a discriminative linear representation for intrinsic low-dimensional structures from high-dimensional data, they propose an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class, known as <em><strong>rate reduction</strong></em>.</p>
<h3 id="Measure-of-Compactness-for-Linear-Representation"><a href="#Measure-of-Compactness-for-Linear-Representation" class="headerlink" title="Measure of Compactness for Linear Representation"></a>Measure of Compactness for Linear Representation</h3><p><em><strong>Rate distortion</strong></em> measures the “compactness” of a random distribution: Given a random variable $z$ and a prescribed precision $\epsilon &gt; 0$, the rate distortion $R(z, \epsilon)$ is the minimal number of binary bits needed to encode $z$ such that the expected decoding error is less than $\epsilon$, i.e., the decoded $\hat{z}$ satisfied $\mathbb{E}[| z - \hat{z} | _2] \leq \epsilon$. </p>
<p><em><strong>Rate distortion for finite samples on a subspace.</strong></em> The compactness of learned features as a whole can be measured in terms of the average coding length per sample (as the sample size m is large), a.k.a. the coding rate subject to the distortion $\epsilon$:</p>
<p>$$<br>\begin{equation}<br>R(\mathbf{Z}, \epsilon) &#x3D; \frac{1}{2} \log \det (\mathbf{I} + \frac{n}{m \epsilon^2} \mathbf{Z}\mathbf{Z}^\star)<br>\end{equation}<br>$$</p>
<p><em><strong>Rate distortion of samples on a mixture of subspaces.</strong></em> We may partition the data (representation) $\mathbf{Z}$ into multiple subsets: $\mathbf{Z} &#x3D; \mathbf{Z}^1 \cup \mathbf{Z}^2 \ldots \mathbf{Z}^k$ with each $\mathbf{Z}^j$ containing samples in one low-dimensional subspace. Let $\Pi &#x3D; { \Pi ^ j \in \mathbb{R}^{m \times m} }_{j&#x3D;1}^k$ be a set of diagonal matrices whose diagonal entries encode the membership of the $m$ samples in the $k$ classes. The average number of bits per sample (the coding rate) is </p>
<p>$$<br>\begin{equation}<br>R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi}) &#x3D; \sum_{j &#x3D; 1} ^k \frac{tr(\mathbf{\Pi})}{2m} \log \det \left( \mathbf{I} + \frac{d}{tr(\mathbf{\Pi}^j)\epsilon ^2} \mathbf{Z}\mathbf{\Pi}^j \mathbf{Z}^\star  \right)<br>\end{equation}<br>$$</p>
<p>When $\mathbf{Z}$ is given, $R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi})$ is a concave function of $\mathbf{\Pi}$. </p>
<h3 id="Principle-of-Maximal-Coding-Rate-Reduction"><a href="#Principle-of-Maximal-Coding-Rate-Reduction" class="headerlink" title="Principle of Maximal Coding Rate Reduction"></a>Principle of Maximal Coding Rate Reduction</h3><p>The learned features should follow the basic rule that <em><strong>similarity contracts and dissimilarity contrasts</strong></em>. To be more precise, a good (linear) discriminative representation $\mathbf{Z}$ of $\mathbf{X}$ is one such that, given a partition $\mathbf{\Pi}$ of $\mathbf{Z}$, achieves a large difference between the coding rate for the whole and that for all the subsets:</p>
<p>$$<br>\begin{equation}<br>\Delta R(\mathbf{Z}, \mathbf{\Pi}, \epsilon) &#x3D; R(\mathbf{Z}, \epsilon) - R_c(\mathbf{Z}, \epsilon | \mathbf{\Pi})<br>\end{equation}<br>$$</p>
<p>If we choose our feature mapping to be $z &#x3D; f(x, \theta)$ (a dnn), the overall process of the feature representation and the resulting rate reduction w.r.t. certain partition $\mathbf{\Pi}$ can be illustrated by the following diagram:</p>
<p>$$<br>\begin{equation}<br>\boldsymbol{X} \stackrel{f(\boldsymbol{x}, \boldsymbol{\theta})}{\longrightarrow} \boldsymbol{Z}(\boldsymbol{\theta}) \stackrel{\Pi, \epsilon}{\longrightarrow} \Delta R(\boldsymbol{Z}(\boldsymbol{\theta}), \boldsymbol{\Pi}, \epsilon)<br>\end{equation}<br>$$</p>
<p><strong>Normalization</strong>. To make the amount of reduction comparable between different representations, we need to normalize the scale of the learned features, either by imposing the Frobenius norm of each class $\mathbf{Z}^j$ to scale with the number of features in $\mathbf{Z}^j \in \mathbb{R}^{n \times m_j} : | \mathbf{Z}^j | _F^2 &#x3D; m_j$ or by normalizing each feature to be on the unit sphere: $z ^i \in \mathcal{S}^{n-1}$. </p>
<p>Once the representations can be compared fairly, our goal becomes to learn a set of features $\mathbf{Z}(\theta) &#x3D; f(\mathbf{X}, \theta)$ and their partition $\mathbf{\Pi}$ such that they maximize the reduction between the coding  rate of all features and that of the sum of features w.r.t. their classes:</p>
<p>$$<br>\begin{equation}<br>\begin{split}<br>\max_{\theta, \Pi} \Delta R(\mathbf{Z}(\mathbf{\theta}), \mathbf{\Pi}, \epsilon) &amp;&#x3D; R(\mathbf{Z}(\mathbf{\theta}), \epsilon) - R_c (\mathbf{Z}(\theta), \epsilon | \mathbf{\Pi}),  \<br> \text{ s.t. } &amp; | \mathbf{Z}^j (\mathbf{\theta}) |_F ^2 &#x3D; m_j , \mathbf{\Pi} \in \Omega<br>\end{split}<br>\end{equation}<br>$$</p>
<p>This is refered to as the principle of maximal coding rate reduction (MC$R^2$). </p>
<h3 id="Properties-of-the-Rate-Reduction-Function"><a href="#Properties-of-the-Rate-Reduction-Function" class="headerlink" title="Properties of the Rate Reduction Function"></a>Properties of the Rate Reduction Function</h3><p>The MC$R^2$ principle is very general and can be applied to representations $\mathbf{Z}$ of any distributions with any attributes $\mathbf{\Pi}$ as long as the rates $R$ and $R_c$ for the distributions can be accurately and efficiently evaluated. </p>
<p>The optimal representation $\mathbf{Z_\star}$ has the following desired properties:</p>
<p><strong>Theorem 1</strong> (Informal Statement) Suppose $\mathbf{Z_\star} &#x3D; \mathbf{Z_\star^1} \cup \cdots \cup \mathbf{Z_\star^k}$ is the optimal solution that maximizes the rate reduction with the rates $R$ and $R_c$. Assume that the optimal solution satisfies $rank(\mathbf{Z_\star^j}) \leq d_j$. We have:</p>
<ul>
<li>Between-class Discriminative: As long as the ambient space is adequately large $(n \geq \sum_{j&#x3D;1}^k d_j)$, the subspaces are all orthogonal to each other, i.e. $(\mathbf{Z_\star^i}) * \mathbf{Z_\star^j} &#x3D; 0$ for $i \neq j.$</li>
<li>Maximally Diverse Representation: As long as the coding precision is adequately high, i.e., $\epsilon^4 &lt; \min_j {\frac{m_j}{m} \frac{n^2}{d_j^2} }$, each subspace achieves its maximal dimension, i.e. $rank(\mathbf{Z_\star^j}) &#x3D; d_j$. In addition, the largest $d_j - 1$ singular values of $\mathbf{Z_\star}^j$ are equal.</li>
</ul>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a id="1">[1]</a><br>@article{chan2020redunet,<br>  title&#x3D;{ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction},<br>  author&#x3D;{Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi, Haozhi and Wright, John and Ma, Yi},<br>  journal&#x3D;{arXiv preprint arXiv:2105.10446},<br>  year&#x3D;{2021}<br>}</p>
<p><a id="2">[2]</a> </p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/rate-reduction/" rel="tag"># rate reduction</a>
              <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/2022/01/blog-post-53/" rel="prev" title="NIPS 2020 Reinforcement Learning Paper: Imitation Learning">
      <i class="fa fa-chevron-left"></i> NIPS 2020 Reinforcement Learning Paper: Imitation Learning
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#ReduNet-1"><span class="nav-number">1.</span> <span class="nav-text">ReduNet [1]</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Principle-of-Maximal-Coding-Rate-Reduction"><span class="nav-number">1.1.</span> <span class="nav-text">The Principle of Maximal Coding Rate Reduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Measure-of-Compactness-for-Linear-Representation"><span class="nav-number">1.1.1.</span> <span class="nav-text">Measure of Compactness for Linear Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Principle-of-Maximal-Coding-Rate-Reduction"><span class="nav-number">1.1.2.</span> <span class="nav-text">Principle of Maximal Coding Rate Reduction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Properties-of-the-Rate-Reduction-Function"><span class="nav-number">1.1.3.</span> <span class="nav-text">Properties of the Rate Reduction Function</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">2.</span> <span class="nav-text">References</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">52</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">116</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  















    <div id="pjax">
  

  

    </div>
</body>
</html>
