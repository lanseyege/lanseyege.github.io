<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>first-post | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Soft Reinforcement Learning: A Shallow OverviewWhat does “Soft” mean?Soft Q-learning [1] is classical Q-learning with a entropy-regularized item. The “Soft” is relative to conventional approach, which">
<meta property="og:type" content="article">
<meta property="og:title" content="first-post">
<meta property="og:url" content="http://lanseyege.github.io/2021/02/04/first-post/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Soft Reinforcement Learning: A Shallow OverviewWhat does “Soft” mean?Soft Q-learning [1] is classical Q-learning with a entropy-regularized item. The “Soft” is relative to conventional approach, which">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-02-04T00:19:37.000Z">
<meta property="article:modified_time" content="2021-02-04T00:54:51.237Z">
<meta property="article:author" content="Ye Yuan">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://lanseyege.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-first-post" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/02/04/first-post/" class="article-date">
  <time class="dt-published" datetime="2021-02-04T00:19:37.000Z" itemprop="datePublished">2021-02-04</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      first-post
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="Soft-Reinforcement-Learning-A-Shallow-Overview"><a href="#Soft-Reinforcement-Learning-A-Shallow-Overview" class="headerlink" title="Soft Reinforcement Learning: A Shallow Overview"></a>Soft Reinforcement Learning: A Shallow Overview</h1><h2 id="What-does-“Soft”-mean"><a href="#What-does-“Soft”-mean" class="headerlink" title="What does “Soft” mean?"></a>What does “Soft” mean?</h2><p><strong>Soft Q-learning</strong> <a href="#1">[1]</a> is classical Q-learning with a entropy-regularized item. The “Soft” is relative to conventional approach, which uses a “hard” max in Bellman equation. Comparing to “hard” approach, the “soft” can provide some benefits <a href="#2">[2]</a>:</p>
<ul>
<li>Better Exploration </li>
<li>Fine-tuning Maximum Entropy Policies</li>
<li>Compositionality </li>
<li>Robustness</li>
</ul>
<p><strong>Traditional Bellman Equation</strong><br>In MDPs, the action-value function of a state-action pair $(s,a)$ under policy $\pi$, denoted as $Q_\pi (s, a)$, is the expected return when starting from $s$, following action $a$ and $\pi$ thereafter. The  action-value Bellman equation and Bellman optimality equation are defined as: </p>
<p>$$<br>\begin{split}<br>Q(s, a) = \sum_a \pi(a|s) \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q(s^\prime, a)] \<br>Q^\star(s, a) = \max_a \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q^\star(s^\prime, a)] \<br>\end{split}<br>$$</p>
<p>The optimal policy can be derived by:<br>\begin{equation}<br>\pi(s) = \arg \max_a Q^\star (s, a)<br>\end{equation}</p>
<h2 id="Maximum-Entropy-Policies"><a href="#Maximum-Entropy-Policies" class="headerlink" title="Maximum Entropy Policies"></a>Maximum Entropy Policies</h2><p><strong>Entropy and Max Entropy Principle</strong><br>Entropy is an old concept in physicals that is used to describe the randomness in environments. The greater the entropy, the more random the actions the policy gives. The discrete form of entropy is: </p>
<p>$$<br>\begin{equation}<br>H(X) = \mathbb{E}<em>X [I(x)] = - \sum</em>{x \in X} p(x) \log p(x)<br>\end{equation}<br>$$</p>
<p>Similarity, the entropy term for policy has this form:</p>
<p>\begin{equation}<br>H(\pi(\cdot | s_t)) = - \sum_{a\in A} \pi(a|s_t) \log \pi(a|s_t)<br>\end{equation}</p>
<p>The entropy term of policy can help the policy to increase the expoloration ability, by adding more possibilities to some rare actions. That could avoid the agent get stuck into local optimum and attain global optimum, to some extend. The idea of learning such maximum entropy model has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics <a href="#1">[1]</a>. The <strong>principle of maximum entropy</strong> states that the probability distribution with the highest entropy, is the one that best represents the current state of knowledge in the context of precisely stated prior data <a href="#4">[4]</a>. </p>
<p><strong>Stochastic Policy and Maximum Entropy Policies</strong><br>Conventional RL approach is to specify a unimodal policy distribution, centered at the maximal Q-value and extending to the neighbouring actions to provide noise for exploration. Usually, there would be a sampling process which employ a Gaussian distribution, $\pi(a_t|s_t) = N(u(s_t), \sum)$. This would make the policy ignore the action area with low Q-value and thus reduce the exploration ability. Do we have a method to solve this problem? Yes! An obvious solution is to ensure the agent explores all promising states while prioritizing the more promising ones. One way to formalize this idea is to define the policy directly in terms of exponentiated Q-values: $\pi(a\mid s) \propto\exp Q(s, a)$. This density has the form of the Boltzman distribution. What’s more, the policy defined through the energy form is an optimal solution for the maximum-entropy RL objective</p>
<p>$$<br>\begin{equation}<br>\pi^\star_\text{MaxEnt} = \arg \max_\pi \mathbb{E}<em>\pi [\sum</em>{t=0}^T r_t + H(\pi(\cdot | s_t))]<br>\end{equation}<br>$$</p>
<p>As showed in the paper, the optimal policy for this equation is given by </p>
<p>$$<br>\begin{equation}<br>\pi_\text{MaxEnt}^\star (a_t | s_t) = \exp (\frac{1}{\alpha}(Q_\text{soft}^\star(s_t, a_t) - V_\text{soft}^\star(s_t)))<br>\end{equation}<br>$$</p>
<p>where soft Q-function $Q^\star_\text{soft}(s_t, a_t)$ and soft value function $V^\star_\text{soft}(s_t)$ are defined by:</p>
<p>$$<br>\begin{split}<br>Q^\star_\text{soft}(s_t, a_t) &amp;= r_t + \mathbb{E}<em>{(s</em>{t+1}, \cdots)\sim \rho_\pi}[\sum_{l=1}^\infty \gamma^l (r_{t+l} + \alpha H(\pi^\star_\text{MaxEnt}(\cdot | s_{t+l})))] \<br>V^\star_\text{soft}(s_t) &amp;= \alpha \log \int_{A} \exp(\frac{1}{\alpha}Q^\star_\text{soft}(s_t, a_t) - V^\star_\text{soft}(s_t)) \<br>\end{split}<br>$$</p>
<p>The idea of learning such maximum entropy models has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics.</p>
<p><strong>Soft Bellman Equation and Soft Q-Learning</strong><br>The soft Bellman equation can be obtained</p>
<p>$$<br>\begin{equation}<br>Q(s_t, a_t) = \mathbb{E}[r_t + \gamma \text{softmax}<em>a Q(s</em>{t+1}, a)]<br>\end{equation}<br>$$</p>
<p>where $\text{softmax}_a f(a) = \log \int \exp f(a)da$. The soft Bellman equation satisfy contraction property, that means it can convergence to optimal value. We can adopt conventional algorithms for the soft. </p>
<p><strong>Two Challenges in Continuous Domains</strong><br>The first challenge is exact dynamic programming is infeasible, since the soft Bellman equation needs to hold for every state and action, and the softmax involves integrating over the entire action space. The solution is to employ expressive neural network function approximators.  </p>
<p>The second is the optimal policy is defined by an intractable energy-based distribution, which is difficult to sample from. To address this problem, it can employ approximate inference techniques, such as MCMC. TO accelerate inference, the amortized Stein variational gradient descent <a href="#3">[3]</a> can be used to train an inference network to generate approximate samples.  </p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a id="1">[1]</a><br>@article{haarnoja2017reinforcement,<br>  title={Reinforcement learning with deep energy-based policies},<br>  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},<br>  journal={arXiv preprint arXiv:1702.08165},<br>  year={2017}<br>} </p>
<p><a id="2">[2]</a><br><a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/</a></p>
<p><a id="3">[3]</a><br>Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016.</p>
<p><a id="4">[4]</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df">https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://lanseyege.github.io/2021/02/04/first-post/" data-id="ckkq5e42j0000scu10ce68nu2" data-title="first-post" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/02/04/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">February 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/02/04/first-post/">first-post</a>
          </li>
        
          <li>
            <a href="/2021/02/04/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Ye Yuan<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>