---
useMath: true
title: 'Graph Convolutional Networks'
mathjax: true
date: 2020-11-05
permalink: /posts/2020/11/blog-post-13/
tags:
  - Graph
  - Convolution
  - Laplacian
---
# Graph Convlutional Network

<!-- more -->

## Classical Wavelet Transform (CWT)
In general, the CWT will be generated by the choice of a single "mother" wavelet $\psi$. Wavelets at different locations and spatial scales are formed by translating and scaling the mother wavelet. We write this by

$$
\psi_{s,a}(x) = \frac{1}{s}\psi(\frac{x-1}{s})
$$

This scaling convention preserves the $L^1$ norm of the wavelets. $s>0$. For a given signal $f$, the wavelet coefficient at scale $s$ and location $a$ is given by the inner product of $f$ with the wavelet $\psi_{s,a}$, i.e.

$$
W_f(s,a) = \int_{-\infty}^{\infty}\frac{1}{s}\psi^\star(\frac{x-a}{s})f(x)dx
$$

The CWT satisfies the admissibility condition

$$
\int_0^\infty \frac{\lvert \hat{\psi}(\omega)\rvert^2}{\omega}d\omega = C_{\psi} < \infty
$$

### Comparisons with Fourier transform (continuous-time) [[6]](#6)
The wavelet transform is often compared with the Fourier transform, in which signals are represented as a sum of sinusoids. In fact, the Fourier transform can be viewed as a special case of the continuous wavelet transform with the choice of the mother wavelet $\phi(t) = e ^{-2\pi i t}$. ***The main difference*** in general is that wavelets are localized in both time and frequency whereas the standard Fourier transform is only localized in frequency. 

***Conjugate Transpose***: The conjugate transpose (or Hermitian transpose) of an m-by-n matrix $\mathbf{A}$ with complex entries, is the n-by-m matrix obtained from $\mathbf{A}$ by taking the transpose and then taking the complex conjugate of each entry (the complex conjugate of $a + ib$ being $a - ib$, for real numbers a and b). It is often denoted as $\mathbf{A}^H$ or $\mathbf{A}^\star$. For real matrices, the conjugate transpose is just the transpose, $\mathbf{A}^H = \mathbf{A}^\top$. 

***Complex Conjugate***: the complex conjugate of a complex number is the number with an equal real part and an imaginary part equal in magnitude, but opposite in sign. Given a complex number $z = a + bi$, the complex conjugate of $z$, often denoted as $\bar{z} = a - bi$. For matrices of complex numbers, $\bar{\mathbf{AB}} = (\bar{\mathbf{A}})(\bar{\mathbf{B}})$, where $\bar{\mathbf{A}}$ represents the element-by-element conjugation of $\mathbf{A}$. Contrast this to the property $(\mathbf{AB})^\star = \mathbf{B}^\star \mathbf{A}^\star$, where $\mathbf{A}^\star$ represents the conjugate transpose of $\mathbf{A}$. 

## Vanilla Graph Convlutional Network
A multi-layer Graph Convolutional Network (GCN) with the following layer-wise propagation rule:

$$
H^{(l+1)} = \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
$$

Here, $\tilde{A} = A + I_N$ is the adjacency matrix of the undirected graph $\mathcal{G}$ with added self-connections. $I_N$ is the identity matrix, $ \tilde{D} _{ii} = \sum_j \tilde{A} _{ij}$ and $W^{(l)}$ is a layer-specific trainable weight matrix. $\sigma(\cdot)$ denotes an activation function, such as $\text{ReLU}(\cdot) = \text{max} (0, \cdot) . \quad H^{(l)} \in \mathbb{R}^{N\times D}$ is the matrix of activations in the $l^\text{th}$ layer; $H^{(0)} = X$.

### Spectral Graph Convolutions
We consider spectral convolutions on graphs defined as the multiplication of a signal $x \in \mathbb{R}^N$ (a scalar for every node) with a filter $g_\theta = \text{diag}(\theta)$ parameterized by $\theta \in \mathbb{R}^N$ in the Fourier domain, 

$$
g_\theta \star x = U g_\theta U^\top x
$$

where $U$ is the matrix of eigenvectors of the normalized graph Laplacian $L = I_N - D^{-\frac{1}{2}}AD^{-\frac{1}{2}} = U\Lambda U^\top$, with a diagonal matrix of its eigenvalues $\Lambda$ and $U^\top x$ being the graph Fourier transform of $x$. ***Drawbacks*** of this formulation are:

  - Computationally expensive, as multiplication with the eigenvector matrix $U$ is $\mathcal{O}(N^2)$;
  - Computing the eigendecompositon of $L$ in the first place might be prohibitively expensive for large graphs


### Graph Fourier Transform 
We are interested in processing signals defined on undirected and connected graphs $\mathcal{G} = (\mathcal{V}, \mathcal{E}, W)$, where $\mathcal{V}$ is a finite set of $\lvert \mathcal{V} \rvert = n$ vertices, $\mathcal{E}$ is a set of edges and $W \in \mathbb{E}^{n\times n}$ is a weighted adjacency matrix encoding the connection weight between two vertices. A signal $x : \mathcal{V} \rightarrow \mathbb{R}$ defined on the nodes of the graph may be regarded as a vector $ x \in \mathbb{R}^n $ where $x_i$ is the value of $x$ at the $i^{\text{th}}$ node. An essential operator in spectral graph analysis is the graph Laplacian, which combinatorial definition is $L = D - W \in \mathbb{R}^{n \times n}$ where $D \in \mathbb{R}^{n \times n}$ is the diagonal degree matrix with $D_{ii} = \sum_j W_{ij}$, and normalized definition is $L = I_n - D^{-1/2}W D^{-1/2}$ where $I_n$ is the identity matrix. As $L$ is a real symmetric positive semidefinite matrix, it has a complete set of orthonormal eigenvectors $$\left\{ u_l \right\} _{l=0}^{n-1} \in \mathcal{R}^n$$, known as the ***graph Fourier modes***, and their associated ordered real nonnegative eigenvalues $$\left\{ \lambda_l \right\} _{l=0}^{n-1}$$, identified as the ***frequencies of the graph***. The Laplacian is indeed diagonalized by the Fourier basis $$U = \left[u_0, \ldots, u _{n-1}\right] \in \mathbb{R}^{n \times n}$$ such that $$L = U\Lambda U^\top$$ where $$\Lambda=\operatorname{diag}\left(\left[\lambda_{0}, \ldots, \lambda_{n-1}\right]\right) \in \mathbb{R}^{n \times n}$$. The ***graph Fourier transform*** of a signal $$x \in \mathbb{R}^n$$ is then defined as $$\hat{x} = U^\top x \in \mathbb{R}^n$$, and its ***inverse*** as $$x = U \hat{x}$$. As on Euclidean spaces, that transform enables the formulation of fundamental operations such as filtering. [[1]](#1)

### Spectral filtering of graph signals
As we cannot express a meaningful translation operator in the vertex domain, the **convolution operator on graph** $\star_{\mathcal{G}}$ is defined in the Fourier domain such that $ x \star_{\mathcal{G}} y = U((U^\top x) \odot (U\top y)) $, where $ \odot $ is the element-wise Hadamard product. It follows that a signal $x$ is filtered by $g_\theta$ as 

$$
y = g_\theta(L)x = g_\theta(U\Lambda U^\top) x = U g_\theta (\Lambda) U^\top x.
$$

A non-parametric filter, i.e. a filter whose parameters are all free, would be defined as 

$$
g_\theta (\Lambda) = \operatorname{diag}(\theta)
$$

where the paramter $\theta \in \mathbb{R}^n$ is a vector of Fourier coefficients. 

## Another Perspective to GCN [[2]](#2)

 - $A \in \mathbb{R}^{N\times N}$ adjacency matrix of graph $G$
 - $D \in \mathbb{R}^{N\times N}$ degree matrix 
 - $L \in \mathbb{R}^{N\times N}$ Laplacian matrix
 - $g \in \mathbb{R}^{N}$ convolutional kernel filter
 - $x \in \mathbb{R}^{N}$ signal on graph 
 
There are usually three kinds of Laplacian matrix $L$:
 - Unnormalized Laplacian: $L_{(1)} = D - A$
 - Symmetric normalized Laplacian: $L_{(2)} = D^{-\frac{1}{2}}L_{(1)}D^{-\frac{1}{2}} = I - D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$
 - Random walk normalized Laplacian: $L_{(3)} = D^{-1}L_{(1)} = I - D^{-1}A$

### Graph Convolution 

$$
g \otimes x = U((U^\top g) \odot (U^\top x))
$$

$\otimes$ is convolutional operator, $\odot$ is Hadamard product. Laplacian matrix $L = U\Lambda U^\top, U \in \mathbb{R}^{N\times N}$ is L's eigenvectors, $\Lambda \in \mathbb{R}^{N \times N }$ is eigenmatrix. $U^\top g$ and $U^\top x$ is doing Fourier transform for $g$ and $x$, respectively. 

### Spectral Graph Convolution 

$$
g \otimes x = U((U^\top g) \odot (U^\top x)) = U \hat{g}U^\top x = U\hat{g}(\Lambda) U^\top x
$$

where 

$$
\hat{g} = \hat{g}(\Lambda) = \operatorname{diag}(\hat{g}(\lambda_l)) = \left(\begin{array}{llll}
\hat{g}\left(\lambda_{1}\right) & & \\
& \ddots & \\
& & \hat{g}\left(\lambda_{n}\right)
\end{array}\right) 
$$ 

is convolutional kernel $g$'s Fourier tranform $U^\top g$'s triangle form. $\hat{g}(\lambda_l) = \sum_{i=1}^N u_l(i)g(i) = u_l^{\top} g$.

### Paramters Graph Convolution [[3]](#3)

$$
g \otimes x = U g_\theta U^\top x
$$

This is replacement of $\operatorname{diag}(\hat{g}(\lambda_l))$ with $\operatorname{diag}(\theta_l)$. And $\theta$ is the parameters we aims to learn. Drawbacks:
 - Huge computation, too complex
 - Amount of parameters is $N$
 - Convolution kernel is not localized connected


### Polynomail Parameter Graph Convolution [[4]](#4)

$$
g \otimes x = \sum_{k=0}^{K} \alpha_k L^k x
$$

This smartly replace $\hat{g}(\Lambda)$ with $$\hat{g}_{\alpha}(\Lambda) = \sum_{k=0}^K \alpha_k \Lambda^k$$, where $\hat{g}(\lambda_l)$ is replaced by $$\hat{g}_{\alpha}(\lambda_l) = \sum_{k=0}^K \alpha_k \lambda_l^k$$. 

 - Reduce the parameter of convolution kernels from $N$ to $K+1$
 - Convolutional Kernal has localized property
 - $K$ is convolutional kernel's reaction range
 - but, calculation is still highly complex 

### Chebyshev Polynomial Graph Convolution [[5]](#5)

$$
g \otimes x = \sum_{k=0}^K \beta_k T_k (\hat{L})x
$$

where $\hat{L} = \frac{2}{\lambda_{\max}}L - I_N$. This consider the Chebyshev polynomial $T_k(x)$'s K-cut to approximate $\hat{g}(\Lambda)$: $$\hat{g}(\Lambda) \approx \hat{g}_{\beta}(\hat{\Lambda}) = \sum_{k=0}^{K}\beta_k T_k (\hat{\Lambda})$$, where $$\hat{\Lambda}=\frac{2}{\lambda_{\max}}\Lambda - I_N$$, $$\beta_k$$ is Chebyshev coefficient, which is paramter in graph network. The Chebyshev polynomial can be defined recursively: $T_k(x) = 2x T_{k-1}(x) - T_{k-2}(x)$, where $T_0 (x) = 1$ and $T_1 (x) = x$. So we can get: 

$$
g \otimes x = U\hat{g}_{\beta}(\hat{\Lambda})U^\top x = U[\sum_{k=0}^{K} \beta_k T_k (\hat(\Lambda))] U^\top x = \sum_{k=0}^K \beta_k U T_k (\hat{\Lambda})U^\top x = \sum_{k=0}^{K} \beta_k T_k(\hat{L})x 
$$

 - similar to above, convolutional operator don't rely on whole graph, but on K-neighbors to center
 - avoid eigen decomposition of Laplacian matrix
 - but still perform K-order calculation on $\hat{L}$

### One-order Chebyshev Polynomial Graph Convolution [[6]](#6)

$$
g \otimes x = \gamma \hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} x
$$

This is a special case in above equation that we set $K=1$. For simplification, we set $\lambda_\max \approx 2$, we have 

$$
g \otimes x \approx \beta_0 x + \beta_1 (L - I_N) x = \beta_0 x - \beta_1 D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x 
$$

Further step, we set $\beta_0 = -\beta_1 = \gamma$, we can get:

$$
g \otimes x \approx \beta_0 x - \beta_1 D^{-\frac{1}{2}} A D^{-\frac{1}{2}}x = \gamma (I_N + D^{-\frac{1}{2}} A D^{-\frac{1}{2}}) x
$$

This operator will fall into number unstable and gradient issues, so it proposes renormalization trick: 

$$
I_N + D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}}
$$

where $\hat{A} = A + I_N$, and $$\hat{D}_{ii} = \sum_j \hat{A}_{ij}$$. Here $$\hat{A}$$ and $$\hat{D}$$ is the adjacency matrix and degree matrix adding self-loop. Finally, we get $$g \otimes x = \gamma \hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}x$$. 

Next, we can get:

$$
Z = \hat{D}^{-\frac{1}{2}} \hat{A} \hat{D}^{-\frac{1}{2}} X W = \tilde{A}XW
$$

where $X \in \mathbb{R}^{N\times C}$ is representation of input, and $W \in \mathbb{R}^{C\times F}$ is paramter matrix, which is the part to learn in networks, $Z \in \mathbb{R}^{N \times F}$ is corresponding convolutional result. 

### References
<a id="1">[1]</a> 
@inproceedings{defferrard2016convolutional,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle={Advances in neural information processing systems},
  pages={3844--3852},
  year={2016}
}

<a id="2">[2]</a> 
https://zhuanlan.zhihu.com/p/126687306

<a id="3">[3]</a> 
@article{bruna2013spectral,
  title={Spectral networks and locally connected networks on graphs},
  author={Bruna, Joan and Zaremba, Wojciech and Szlam, Arthur and LeCun, Yann},
  journal={arXiv preprint arXiv:1312.6203},
  year={2013}
}

<a id="4">[4]</a> 
@inproceedings{defferrard2016convolutional,
  title={Convolutional neural networks on graphs with fast localized spectral filtering},
  author={Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  booktitle={Advances in neural information processing systems},
  pages={3844--3852},
  year={2016}
}

<a id="5">[5]</a> 
@article{hammond2011wavelets,
  title={Wavelets on graphs via spectral graph theory},
  author={Hammond, David K and Vandergheynst, Pierre and Gribonval, R{\'e}mi},
  journal={Applied and Computational Harmonic Analysis},
  volume={30},
  number={2},
  pages={129--150},
  year={2011},
  publisher={Elsevier}
}

<a id="6">[6]</a> 
@article{kipf2016semi,
  title={Semi-supervised classification with graph convolutional networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}

<a id="7">[7]</a> 
https://en.wikipedia.org/wiki/Wavelet
