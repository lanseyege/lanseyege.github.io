---
useMath: true
title: 'Notes on book "Measure Theory (2nd ed.) - Cohn Donald L." '
mathjax: true
date: 2021-01-17
permalink: /posts/2021/01/blog-post-30/
tags:
  - Probability
  - Large Number
  - Convergence
---

# Measure Thoery: Chp 10. Probability

<!-- more -->

## 10.1 Basics

A **probability space** is a measure space $(\Omega, \mathscr{A} ,P)$ such that $P(\Omega) = 1$. The elements of $\Omega$ are called the **elementary outcomes** or the **sample points** of our experiment, and the members of $\mathscr{A}$ are called **events**. If $A \in \mathscr{A} $, then $P(A)$ is the **probability** of the event A.

A **real-valued random variable** on a probability space $(\Omega, \mathscr{A} ,P)$ is an $\mathscr{A}$-measurable function from $\Omega$ to $\mathbb{R}$. More generally, a **random variable** with values in a measurable space $(S, \mathscr{B})$ is a measurable function from $(\Omega, \mathscr{A} ,P)$ to $(S, \mathscr{B})$. 

If a real-valued random variable $X$ on a probability space $(\Omega, \mathscr{A} ,P)$ is integrable with respect to $P$, then its expected value, or **expectation**, written E(X), is the integral of X with respect to P.


One calls the expected value of $(X −E(X))^2$ the variance of $X$; it gives a measure of the amount by which the values of $X$ differ from the expected value of $X$. 


Function $F_X : \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
F_X (x) = P_X((−\infty,x]) = P(X\leq x)
$$

The function $F_X$ is called the **cumulative distribution function**.

## 10.2 Laws of Large Numbers 

Let $X$ and $X_1, X_2, \ldots$ be random variables on the probability space $(\Omega, \mathscr{A} ,P)$. Then $\{X_n\}$ is said to **converge in probability** to $X$ if

$$
\lim_n P(\lvert X_n - X \rvert > \epsilon) = 0
$$

holds for each positive number $\epsilon$ and to **converge almost surely** to $X$ (or to converge a.s. to X) if 

$$
P(X = \lim_n X_n) = 1
$$

In other words, $\{X_n\}$ converges to $X$ in probability if it converges to $X$ in measure, and $\{X_n\}$ converges to $X$ almost surely if it converges to $X$ almost everywhere. 

***Theorem 10.2.1 (Weak Law of Large Numbers).*** Let $\{X_n\}$ be a sequence of independent identically distributed real-valued random variables with finite second moments. For each $n$ let $S_n = X_1 + \cdots + X_n$. Then $S_n /n$ converges to $E(X_1)$ in probability.


***Theorem 10.2.5 (Strong Law of Large Numbers).*** Let $\{Xn\}$ be a sequence of independent identically distributed random variables with finite expected values. For each $n $ let $S_n = X_1 + \cdots +X_n$. Then $\{S_n / n\}$ converges to $E(X_1)$ almost surely.


***Proposition 10.2.6 (Kolmogorov’s Inequality).*** Let $X_1, X_2, \cdots , X_n$ be independent random variables, each of which has mean $0$ and a finite second moment, and for each $i$ let $S_i = X_1+ \cdots +X_i$. Then

$$
P(\max_{1\leq i \leq n} \lvert S_i \rvert > \epsilon) \leq (\frac{1}{\epsilon^2})\sum_{i=1}^n E(X_i^2)
$$

holds for each positive $\epsilon$. 


***Theorem 10.2.8 (Converse to the Strong Law of Large Numbers).*** Let $\{X_n\}$ be a sequence of independent identically distributed random variables that do not have finite expected values. For each $n$ let $S_n = X_1+ \cdots +X_n$. Then $\lim \sup_n \lvert S_n/n \rvert =+\infty$ almost surely.


## 10.3 Convergence in Distribution and the Central Limit Theorem 

***Theorem 10.3.16 (Central Limit Theorem).*** Let $X_1, X_2, \ldots$ be a sequence of independent identically distributed random variables, with common mean $\mu$ and variance $\sigma^2$, and for each $n$ let $S_n = X_1 + \cdots +X_n$. Then the normalized sequence $\{(S_n − n \mu)/\sigma \sqrt n \}$ converges in distribution to a normal (i.e., Gaussian) distribution with mean $0$ and variance $1$.

## 10.4 Conditional Distributions and Maringales 

***Theorem 10.4.8 (Doob’s Martingale Convergence Theorem).*** Let $(\Omega, \mathscr{A} ,P)$ be a probability space, and let $(\{X_n\}^\infty _{n=0}, \{F_n\}^\infty_{n=0})$ be a submartingale on $(\Omega, \mathscr{A} , P)$ such that $\sup_n E(X^+ _n ) < +\infty$. Then the limit $\lim_n X_n$ exists almost surely, and $E(\lvert \lim_n X_n \rvert) < +\infty$.


## 10.5 Brownian Motion

Suppose that $(\Omega,\mathscr{A} ,P)$ is a probability space and that $T$ is either $[0,1]$ or $[0,+\infty)$. A stochastic process $\{X_t\} _{t\in T}$ with values in $\mathbb{R}$ is a **Brownian motion** if
  - $X_0(\omega) = 0, \forall \omega \in \Omega$,
  - for each choice of $t_0, t_1, \ldots, t_n \in T$ such that $t_0 < t_1 < \cdots < t_n$ the increments $X_{t_i} - X_{t_{i-1}}, i = 1, \ldots, n$, are independent, with $X_{t_i} - X_{t_{i-1}}$ having distribution $N(0, t_i - t_{i-1})$, that is, a normal distribution with mean $0$ and variance $t_i - t_{i-1}$, and
  - for each $\omega \in \Omega$ the function $X_{\bullet}(\omega): T\rightarrow \mathbb{R}$ defined by $t \rightarrow X_t(\omega)$ is continuous.  

***Theorem 10.5.1.*** Let $T = [0, 1]$. Then a one-dimensional Brownian motion with parameter set $T$ exists. That is, there exist a probability space $(\Omega, \mathscr{A} ,P)$ and random variables $X_t, t \in T$, on $\Omega$ such that the stochastic process $\{X_t\} _{t \in T}$ is a Brownian motion.


## 10.6 Construction of Probability Measures 

***Theorem 10.6.2 (Kolmogorov Consistency Theorem).*** Let $I$ be a nonempty set, let $\{(\Omega_i, \mathscr{A}_i)\}_{i \in I} $ be an indexed family of measurable spaces, and let $I$ be the collection of all nonempty finite subsets of $I$. As in the discussion above, define product measurable spaces $(\Omega, \mathscr{A} )$ and $\{(\Omega_{I_0} , \mathscr{A}_{I_0})\}_{I_0 \in \mathscr{I}}$ , plus projections $X_{I_0} : \Omega \rightarrow \Omega_{I_0}$ and $\text{proj}_{I_2, I_1} : \Omega_{I_1} \rightarrow \Omega_{I_2}$, where $I_0 , I_1, I_2 \in \mathscr{I}$ and $I_2 \in I_1$. Let $\{P_{I_0} \}_{I_0 \in \mathscr{I}}$ be an indexed family of probability measures on the spaces $\{(\Omega_{I_0} , \mathscr{A}_{I_0})\}_{I_0 \in \mathscr{I}}$. If
  - the measurable spaces $\{ (\Omega_i, \mathscr{A}_i)\}$ are all standard, and 
  - the measures $\{ P_{I_0}\}_{I_0 \in \mathscr{I}} $ are consistent, in the sense that they satisfy condition (2)

then there is a unique probability measure $P$ on $(\Omega, \mathscr{A} )$ such that for each $I_0$ in $\mathscr{I}$ the distribution of $X_{I_0}$ is $P_{I_0}$.

### References
<a id="1">[1]</a> 
@book{cohn2013measure,
  title={Measure theory},
  author={Cohn, Donald L},
  year={2013},
  publisher={Springer}
}
