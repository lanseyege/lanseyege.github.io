---
useMath: true
title: 'Convergence of Gradient Descent Methods'
mathjax: true
date: 2021-01-24
permalink: /posts/2021/01/blog-post-33/
tags:
  - Gradient
  - Momentum
  - Nesterov
  - Convergence
---

# Gradient Descent

<!-- more -->


Consider optimization problem:

$$
\min f_w(x) 
$$

## Stochastic Gradient Descent

Suppose $w$ is the parameter. The Stochastic Gradient Descent method is simplied as this:

$$
w_{t+1} = w_t - \eta \nabla f_w
$$

**Theorem** Suppose the function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is convex and differentiable, and that its gradient is Lipschitz continuous with constant $L > 0$, i.e. we have that $\| \nabla f(x) - \nabla f(y) \|_2 \leq L \| x - y \| _2$ for any $x, y$. Then if we run gradient descent for $k$ iterations with a fixed step size $ t \leq 1 / L$, it will yield a solution $f^{(k)}$ which satisfies

$$
f(x^{(k)}) - f(x^\star) \leq \frac{\| x^{(0)} - x^\star \|_2 ^2}{2tk} ,
$$

where $f(x^\star)$ is the optimal value. Intuitively, this means that gradient descent is guaranteed to converge and that it converges with rate $O(1/k)$. 

***Proof:*** Our assumption that $\nabla f$ is Lipschitz continuous with constant $L$ implies $\nabla^2 f(x) \preceq L I$, or equivalently that $\nabla^2 f(x) - LI$ is a negative semidefinite matrix. Using this fact, we can perform a quadratic expansion of $f$ around $f(x)$ and obtain the following inequality: 

$$
\begin{array}{ll}
f(y) &\leq f(x) + \nabla f(x)^\top (y-x) +\frac{1}{2} \nabla^2 f(x) \| y - x \| _2^2 \\
&\leq f(x) + \nabla f(x)^\top (y - x) + \frac{1}{2} L \| y - x \| _2^2 \\
\end{array}
$$

Now let's plug in the gradient descent update by letting $ y = x^+ = x - t \nabla f(x)$. We then get:

$$
\begin{array}{ll}
f(x^+) & \leq f(x) + \nabla f(x)^\top (x^+ - x) + \frac{1}{2} L \| x^+ - x \| _2^2 \\
&= f(x) + \nabla f(x)^\top (x - t\nabla f(x) - x) + \frac{1}{2} L \| x - t \nabla f(x) - x \|_2^2 \\
&= f(x) - \nabla f(x)^\top t \nabla f(x) + \frac{1}{2} L \| t \nabla f(x) \| _2^2 \\ 
&= f(x) - t \| \nabla f(x) \| _2^2 + \frac{1}{2} L t^2 \| \nabla f(x) \| _2^2 \\
&= f(x) - (1 - \frac{1}{2} L t) t \| \nabla f(x) \| _2^2 \\
\end{array}
$$

Using $t \leq 1 / L$, we know that $-(1 - \frac{1}{2}Lt) = \frac{1}{2}Lt - 1 \leq \frac{1}{2}L(1/L) - 1 = -\frac{1}{2}$. Plugging this in to above, we can conclude the following:

$$
f(x^+) \leq f(x) - \frac{1}{2} t \| \nabla f(x) \| _2^2
$$

Since $\frac{1}{2} t \| \nabla f(x) \| _2^2$ will always be positive unless $\nabla f(x) = 0$, this inequality implies that the objective function value strictly decreases with each iteration of gradient descent until it reaches the optimal value $f(x) = f(x^\star)$. Note that this convergence result only holds when we choose $t$ to be small enough, i.e., $t \leq 1 /L$. This explains why we observe in practice that gradient descent diverges when the step size is too large. 

Next, we can bound $f(x^+)$, the objective value at the next iteration, in terms of $f(x^\star)$, the optimal objective value. Since $f$ is convex, we can write 

$$
\begin{array}{ll}
f(x^\star) &\geq f(x) + \nabla f(x)^\top (x ^\star - x) \\
f(x) &\leq f(x^\star) + \nabla f(x)^\top (x  - x^\star) \\
\end{array}
$$

where the first inequality yields the second through simple rearrangement of terms. Plugging this in to above, we obtain:

$$
\begin{array}{ll}
f(x^+) \leq f(x^\star) + \nabla f(x)^\top (x - x^\star) - \frac{t}{2} \| \nabla f(x) \| _2^2 \\ 
f(x^+) - f(x^\star) \leq \frac{1}{2t} (2t \nabla f(x)^\top (x - x^\star) - t^2 \| \nabla f(x) \| _2^2)  \\
f(x^+) - f(x^\star) \leq \frac{1}{2t} (2t \nabla f(x)^\top (x - x^\star) - t^2 \| \nabla f(x) \| _2^2 - \| x - x^\star \|_2^2 + \| x - x^\star \|_2^2) \\
f(x^+) - f(x^\star) \leq \frac{1}{2t} (\| x - x^\star \| _2^2 - \| x - t \nabla f(x) - x^\star \|_2^2) \\
\end{array}
$$

where the final inequality is obtained by observing that expanding the square of $\| x - t \nabla f(x) - x^\star \| _2^2$ yields $\| x - x^\star \|_2^2 - 2t \nabla f(x)^\top (x - x^\star) + t^2 \| \nabla f(x) \|_2^2$. Notice that by definition we have $x^+ = x - t \nabla f(x)$. Plugging this in to above, yields:

$$
f(x^+) - f(x^\star) \leq \frac{1}{2t} ( \| x - x^\star \|_2^2 - \| x^+ - x^\star \| _2^2) 
$$

This inequality holds for $x^+$ on every iteration of gradient descent. Summing over iterations, we get:

$$
\begin{array}{ll}
\sum_{i=1}^k f(x^{(i)}) - f(x^\star) &\leq \sum_{i=1}^k \frac{1}{2t} (\| x^{(i-1)} - x^\star \|_2^2 - \| x^{(i)} - x^\star \|_2^2) \\
& = \frac{1}{2t} (\| x^{(0)} - x^\star \|_2^2 - \| x^{(k)} - x^\star \|_2^2) \\
& \leq \frac{1}{2t} (\| x^{(0)} - x^\star \| _2^2)
\end{array}
$$

where the summation on the right-hand side disappers because it is a telescoping sum. Finally, using the fact that $f$ decreasing on every iteration, we can conclude that 

$$
\begin{array}{ll}
f(x^{(k)}) - f(x^\star) &\leq \frac{1}{k} \sum_{i=1}^k f(x^{(i)}) - f(x^\star) \\
&\leq \frac{\| x^{(0)} -x^\star \|_2^2}{2tk} \\
\end{array}
$$

where in the final step, we plug in it to get the inequality from - that we were trying to prove. 

**Theorem** Convergence of SGD for strongly convex problem; fixed stepsizes. Under the assumption of $\mu$-strongly convex, $L$-smooth, if $\eta_t = \eta \leq \frac{1}{L c_g}$, then SGD achieves 

$$
\mathbb{E} [F(x^t) - F(x^\star)] \leq \frac{\eta L \sigma_g^2}{2\mu} + (1 - \eta \mu)^t (F(x^0) - F(x^\star))  
$$



## Momentum-based Gradient Descent 

$$
\begin{array}{ll}
u^w_t &= \gamma \cdot u^w_{t-1} + \eta \nabla f_w \\
w_{t+1} &= w_t - u^w_t \\
\end{array}
$$


## Nesterov Accelerated Gradient Descent 

$$
\begin{array}{ll}
w_{la} &= w_t - \gamma \cdot u_{t-1} \\
u_t &= \gamma \cdot u_{t-1} + \eta \nabla f_{w_{la}}\\
w_{t+1} &= w_t - u_t \\
\end{array}
$$


# References

<a id="1">[1]</a> 
https://towardsdatascience.com/manifold-learning-the-theory-behind-it-c34299748fec

<a id="2">[2]</a> 



