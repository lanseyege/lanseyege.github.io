---
useMath: true
title: 'Generative Adversarial Nets'
mathjax: true
date: 2021-07-02
permalink: /posts/2021/07/blog-post-51/
tags:
  - Generative
  - Adversarial
  - GAN
---

# Generative Adversarial Nets [[1]](#1)

This is a new framework for estimating generative models via an adversarial process. 

<!-- more -->
A generative model $G$ that captures the data distribution; a discriminative model $D$ that estimates the probability that a sample came from the training data rather than $G$. In the space of arbitrary functions $G$ and $D$, **a unique solution exists**, with $G$ recovering the training data distribution and $D$ equal to $\frac{1}{2}$ everywhere. 


## Adversarial Nets

**setting** 

  - the generator's distribution $p_g$ over data $x$,
  - a prior on input noise variables $p_z(z)$,
  - $G(z; \theta_g)$: a map from z to data space; where $G$ is differentiable function with parameters $\theta_g$,
  - $D(x; \theta_d)$ represents the probability that $x$ came from the data rather than $p_g$. 
  
We train $D$ to maximize ***the probability of assigning the correct label*** to both training examples and samples from $G$; we simultaneously train $G$ to minimize $\log(1 - D(G(z)))$. 

$$
\begin{equation}
\min_G \max_D V(D, G) = \mathbb{E}_{\boldsymbol{x} \sim p_{\text {data }}(\boldsymbol{x})} [\log D(\boldsymbol{x})] + \mathbb{E}_{z \sim p_z(z)} [\log (1 - D(G(z)))]
\end{equation}
$$

When training, it alternates between $k$ steps of optimizing $D$ and one step of optimizing $G$. 

## Theoretical Results

**Global optimality of $p_g = p_{\text{data}}$**

**Proposition 1**: For $G$ fixed, the optimal discriminator $D$ is 

$$
\begin{equation}
D^*_G(x) = \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}  
\end{equation}
$$

with this, equation $1$ can be reformulated as:

$$
\begin{equation}
\begin{split}
C(G) &= \max_{D} V(G, D)  \\
&= \mathbb{E}_{\boldsymbol{x}\sim p_{\text{data}}} [\log D^*_G(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{z}\sim p_z} [\log (1 - D^*_G(G(z)))] \\
&= \mathbb{E}_{\boldsymbol{x}\sim p_{\text{data}}} [\log D^*_G(\boldsymbol{x})] + \mathbb{E}_{\boldsymbol{x}\sim p_g} [\log (1 - D^*_G(\boldsymbol{x}))] \\
&= \mathbb{E}_{\boldsymbol{x}\sim p_{\text{data}}}[\log \frac{p_{\text{data}}(x)}{p_{\text{data}}(x) + p_g(x)}] + \mathbb{E}_{\boldsymbol{x}\sim p_{g}}[\log \frac{p_{g}(x)}{p_{\text{data}}(x) + p_g(x)}]
\end{split}
\end{equation}
$$

**Theorem 1.** The global minimum of the virtual training criterion $C(G)$ is achieved if and only if $p_g = p_{\text{data}}$. At that point, $C(G)$ achieves the value $-\log 4$. With this, we obtain:

$$
\begin{split}
C(G) &= -\log(4) + \text{KL}\left( p_{\text{data}} \| \frac{p_{\text{data}} + p_g}{2} \right) + \text{KL}\left( p_{g} \| \frac{p_{\text{data}} + p_g}{2} \right) \\
&= -\log(4) + 2 \cdot \text{JSD} (p_{\text{data}} \| p_g)
\end{split}
$$



# Princepled Methods for Training GANs [[2]](#2)

This paper makes theoretical steps towards fully understanding the training dynamics of GANs. 


# References

<a id="1">[1]</a> 
@article{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}


<a id="2">[2]</a> 
@article{arjovsky2017towards,
  title={Towards principled methods for training generative adversarial networks},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.04862},
  year={2017}
}


