---
useMath: true
title: 'Neural Tangent Kernel'
mathjax: true
date: 2021-06-21
permalink: /posts/2021/06/blog-post-50/
tags:
  - Tangent
  - Kernel
  - ANN
---

# Neural Tangent Kernel 

[[1]](#1) proposes a new explanation for neural network. During training, the network function follows a descent along the kernel gradient w.r.t. the Neural Tangent Kernel (NTK).


<!-- more -->

The paper sutdy the network function $f_\theta$ of an ANN. In the limit as the widths of the hidden layers tend to infinity, the network function at initialization, $f_\theta$ converges to a Gaussian distribution. The behavior of ANNs during training is described by a related kernel, which we call the **neural tangent kernel (NTK)**. 

## Neural networks

**Setting:** ANN

  - layers numbered from $0$ (input) to $L$ (output), each containing $n_0, \ldots, n_L$ neurons; 
  - a Lipschitz, twice differentiable nonlinearity function $\sigma : \mathbb{R} \rightarrow \mathbb{R}$, with bounded second derivative;
  - ANN **realization function** $F^{(L)} : \mathbb{R}^P \rightarrow \mathcal{F}$, mapping parameters $\theta$ to functions $f_\theta$ in a space $\mathcal{F}$;
  - for a fixed distribution $p^{in}$ on the input space $\mathbb{R}^{n_0}$, the function space $\mathcal{F}$ is defined as $\left\{ f : \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L}  \right\}$;
  - The seminorm $\left \| \cdot \right\| _{p^{in}}$, defined in terms of the bilinear form

  $$
  \langle f, g\rangle_{p^{i n}} = \mathbb{E}_{x \sim p^{in}} [f(x)^\top g(x)]
  $$

  - the network function is defined by $f_\theta(x) = \tilde{\alpha}^{(L)}(x; \theta)$, where the functions $\tilde{\alpha}^{(l)}(\cdot; \theta) : \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_l}$ (***preactivations***) and $\alpha ^{(l)}(\cdot; \theta) : \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_l}$ (***activations***)  


## Kernel gradient

During training, the network function $f_\theta$ follows **a descent along the kernel gradient with respect to the Neural Tangent Kernel (NTK)**. This makes it possible to study the training of ANNs in the function space $\mathcal{F}$, on which the cost $C$ is convex. 

### **setting** 

  - functional cost $C : \mathcal{F} \rightarrow \mathbb{R}$; the composite cost $C \circ F^{(L)} : \mathbb{R}^P \rightarrow \mathbb{R}$ is in general highly non-convex;  
  - a ***multi-dimensional kernel*** $K$ is a function $\mathbb{R}^{n_0} \times \mathbb{R}^{n_0} \rightarrow \mathbb{R}^{n_L \times n_L}$;

    - this kernel defines a bilinear map on $\mathcal{F}$, taking the expectation over independent $x, x^\prime \sim p^{in}$:

    $$
    \langle f, g\rangle_{K}:=\mathbb{E}_{x, x^{\prime} \sim p^{i n}}\left[f(x)^{T} K\left(x, x^{\prime}\right) g\left(x^{\prime}\right)\right]
    $$

  - $\mathcal{F}^*$ the dual of $\mathcal{F}$ with respect to $p^{in}$; the set of linear forms $\mu : \mathcal{F} \rightarrow \mathbb{R}$ of the form $\mu = \langle d, \cdot \rangle_{p^{in}}$ for some $d \in \mathcal{F}$; $\mu \in \mathcal{F}^*$ , and $\left\{ \mu \right\} = \mathcal{F}^*$.
  - $K_{i, \cdot}(x, \cdot) \in \mathcal{F}$, define a map $\Phi_K : \mathcal{F}^* \rightarrow \mathcal{F}$ mapping a dual element $\mu = \langle d, \cdot \rangle _{p^{in}}$ to the function $f_\mu = \Phi_K(\mu)$ with values:

  $$
  f_{\mu, i}(x) = \mu K_{i, \cdot}(x, \cdot) = \langle d, K_{i, \cdot}(x, \cdot) \rangle _{p^{in}}
  $$

###**gradient**

A finite dataset $x_1, \ldots, x_n \in \mathbb{R}^{n_0}$, the cost functional $C$ only depends on the values of $f \in \mathcal{F}$ at the data points. **The (functional) derivative of the cost $C$ at a point $f_0 \in \mathcal{F}$ can be viewed as an element of $\mathcal{F}^*$, which we write  $\partial_{f}^{i n} C|_{f_{0}}$**. We denote by $d|_{f_0} \in \mathcal{F}$, a corresponding dual element, such that $\partial_{f}^{in} C|_{f_0} = \langle d|_{f_0}, \cdot \rangle _{p^{in}}$. 

***Kernel gradient*** $\nabla_K C|_{f_0} \in \mathcal{F}$ is defined as $\Phi_K( \partial_{f}^{i n} C|_{f_{0}})$. The kernel gradient generalize to values $x$ outside the dataset thanks to the kernel $K$:

$$
\nabla_K C|_{f_0}(x) = \frac{1}{N}\sum_{j=1}^N K(x, x_j) d|_{f_0}(x_j)
$$

### Random functions approximation

**Setting:** A kernel $K$ can be approximated by a choice of $P$ random functions $f^{(p)}$ sampled independently from any distribution on $\mathcal{F}$ whose covariance is given by the kernel $K$:

$$
\mathbb{E}[f_k^{(p)}(x) f_{k^\prime}^{(p)}(x^\prime)] = K_{kk^\prime}(x, x^\prime)
$$

These functions define a random linear parametrization $F^{lin} : \mathbb{R}^P \rightarrow \mathcal{F}$

$$
\theta \rightarrow f^{lin}_\theta = \frac{1}{\sqrt{P}}\sum_{p=1}^P \theta_p f^{(p)}
$$

The partial derivatives of the parametrization are given by

$$
\partial_{\theta_p} F^{lin} (\theta) = \frac{1}{\sqrt{P}} f^{(p)}
$$

Optimizing the cost $C \circ F^{lin}$ lin through gradient descent, the parameters follow the ODE:

$$
\partial_{t} \theta_{p}(t)=-\partial_{\theta_{p}}\left(C \circ F^{l i n}\right)(\theta(t))=-\left.\frac{1}{\sqrt{P}} \partial_{f}^{i n} C\right|_{f_{\theta(t)}^{l i n}} f^{(p)}=-\frac{1}{\sqrt{P}}\left\langle\left. d\right|_{f_{\theta(t)}^{l i n}}, f^{(p)}\right\rangle_{p^{i n}}
$$


## Neural tangent kernel

During training, the network function $f_\theta$ evolves along the negative kernel gradient

$$
\partial_t f_{\theta(t)} = - \nabla_{\Theta^{(L)}}C |_{f_{\theta(t)}}
$$

with respect to the **neural tangent kernel (NTK)** 

$$
\Theta^{(L)}(\theta) = \sum_{p=1}^P \partial_{\theta_p} F^{(L)} (\theta) \otimes \partial_{\theta_p} F^{(L)}(\theta)
$$

Next will show that in the infinite-width limit, the NTK becomes deterministic at initialization and stays constant during training. 

### Initialization

**Proposition 1.** For a network of depth $L$ at initialization, with a Lipschitz nonlinearity $\sigma$, and in the limit as $n_1, \ldots, n_{L-1} \rightarrow \infty$, the ***output functions*** $f_{\theta, k}$, for $k = 1, \ldots, n_L$, tend (in law) to iid centered ***Gaussian processes*** of covariance $\Sigma^{(L)}$. 

**The firstly key result**: in the same limit, the Neural Tangent Kernel (NTK) converges in probability to an explicit deterministic limit. 

**Theorem 1.** For a network of depth $L$ at initialization, with a Lipschitz nonlinearity $\sigma$, an in the limit as the layers width $n_1, \ldots, n_{L-1} \rightarrow \infty$, the NTK $\Sigma^{(L)}$ converges in probability to a deterministic limiting kernel:

$$
\Sigma^{(L)} \rightarrow \Sigma^{(L)}_{\infty} \otimes I d_{n_L}
$$

The scalar kernel $\Sigma^{(L)}_\infty : \mathbb{R}^{n_0} \times \mathbb{R}^{n_0} \rightarrow \mathbb{R}$ is defined recursively. 

### Training 

**Second key result** is that the NTK stays asymptotically constant during training. 

**Theorem 2.** Assume that $\sigma$ is a Lipschitz, twice differentiable nonlinearity function, with bounded second derivative. For any $T$ such that the integral $\int_0 ^T \| d_t \| _{p^{in}} dt$ stays stochastically bounded, as $n_1, \ldots, n_{L-1} \rightarrow \infty$, we have, uniformly for $t \in [0, T]$, 

$$
\Sigma^{(L)}(t) \rightarrow \Sigma^{(L)}_\infty \otimes I d_{n_L}
$$

As a consequence, in this limit, the dynamics of $f_\theta$ is described by the differential equation

$$
\partial_t f_{\theta(t)} = \Phi_{\Sigma^{(L)}_\infty \otimes I d_{n_L}}\left(\langle d_t, \cdot \rangle_{p^{in}}\right)
$$

## Least-squares regression 

Given a goal function $f^*$ and input distribution $p^{in}$, the least-squares regression cost is 

$$
C(f) = \frac{1}{2} \| f - f^*\| ^2 _{p^{in}} = \frac{1}{2}\mathbb{E}_{x \sim p^{in}} [\| f(x) - f^*(x) \|^2 ]
$$



# References

<a id="1">[1]</a> 
@inproceedings{jacot2018neural,
	title="Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
	author="Arthur {Jacot} and Franck {Gabriel} and Cl√©ment {Hongler}",
	booktitle="Advances in Neural Information Processing Systems",
	volume="31",
	pages="8571--8580",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2809090039",
	year="2018"
}

<a id="2">[2]</a> 


