---
title: 'Soft Reinforcement Learning: More Details'
mathjax: true
date: 2020-08-05
permalink: /posts/2020/08/blog-post-2/
tags:
  - soft
  - entropy-regularized
---

# Soft Reinforcement Learning: More Details 

<!-- more -->

Soft Q-learning learns policies for continuous state and actions. The soft Q-learning meets contraction property, hence, it can converge to the optimal policy. Here, we look into the certify process. 


## Soft Q-iteration
***Theorem: soft Q-iteration.*** Let $Q_\text{soft}(\cdot, \cdot)$ and $V_\text{soft}(\cdot)$ be bounded and assume that $\int_A \exp (\frac{1}{\alpha}Q_\text{soft}(\cdot, a^\prime))da^\prime < \infty$ and that $Q_\text{soft}^\star < \infty$ exists. Then the fixed-point iteration

$$
\begin{equation}
Q_\text{soft}(s_t, a_t) \leftarrow r_t + \gamma \mathbb{E}_{s_{t+1}\sim p_s}[V_\text{soft}(s_{t+1})], \forall s_t, a_t
\end{equation}
$$

$$
\begin{equation}
V_\text{soft}(s_t) \leftarrow \alpha \log \int_A \exp (\frac{1}{\alpha}Q_\text{soft}(s_t, a^\prime))da^\prime, \forall s_t
\end{equation}
$$

converges to $Q^\star_\text{soft}$ and $V_\text{soft}^\star$, respectively. 

## "Soft" Bellman Operator
Soft value iteration operator $\mathcal{T}$ is defined as

$$
\begin{equation}
\mathcal{T} Q(s, a) \triangleq r(s, a) + \gamma \mathbb{E}_{s^\prime \sim p_s}[\log \int \exp Q(s^\prime, a^\prime) da^\prime]
\end{equation}
$$

This is a contraction mapping. This result is certified in [[1]](#1), that we can get  $\left\|\mathcal{T} Q_{1}-\mathcal{T} Q_{2}\right\| \leq \gamma \varepsilon=\gamma \| Q_{1}-Q_{2} \mid$. Since the soft Bellman backup is a contraction, the optimal value function is the fixed point of the Bellman backup, and it can be found by optimizing for a Q-function for which the soft Bellman error $\|\mathcal{T}Q-Q\|$ is minimized at all states and actions. 

## Simple from the Soft Q-function 
Soft Q-learning needs to sample from the policy, $\pi(a_t|s_t) \propto \exp (\frac{1}{\alpha}Q_\text{soft}^\theta(s_t, a_t))$, both to take on-policy actions and to generate action samples for estimating the soft value function. Since the form of the policy is so general, the direct sampling is intractable. The paper adopt an approximate way to do this, let's see, Stein variational gradient descent (SVGD) [[3]](#3). ***firstly***, they learn a state-conditioned stochastic neural network $a_t = f^\phi(\xi; s_t)$, parametrized by $\phi$, that maps noise samples $\xi$ drawn from a distribution into unbiased action samples from the target EBM corresponding to $Q_\text{soft}^\theta$. (**secondly,**) The induced distribution of the actions are $\pi^\phi(a_t | s_t)$, the goal is to find parameters $\phi$ so that the induced distribution approximates the energy-based distribution in terms of the KL divergence. **We will list the details of SVGD in another blog.** The paper also indicates the relation of their algos $f^\phi(\xi;s_t)$ and policy-gradient based methods, which you can find more details in
 [[2]](#2) (maybe we will organize these in another blog). 

# Soft Actor-Critic Demystified ([[4]](#4))

***SAC*** is defined for RL tasks involving continuous actions. The biggest feature of SAC is that it uses a modified RL objective function. Instead of only seeking to maximize the lifetime rewards, SAC seeks to also maximize the entropy of the policy. A high entropy in the policy will explicitly encourage exploration, to ensure that it does not collapse into repeatedly selecting a particular action that could exploit some inconsistency in the approximated Q-function. 

## SAC architecture

The optimization objective:

$$
J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t, a_t)\sim \rho_\pi}[r(s_t, a_t) + \alpha \mathcal{H}(\pi(\cdot | s_t))]
$$

SAC makes use of three networks: 

  - a state value function $V$ parameterized by $\psi$
  - a soft Q-function $Q$ parameterized by $\theta$
  - and a policy function $\pi$ parameterized by $\phi$

### We train the value network by minimizing the following error:

$$
J_V(\psi) = \mathbb{E}_{s_t \sim \mathcal{D}}[\frac{1}{2}(V_{\psi}(s_t) - \mathbb{E}_{a_t \sim \pi_\phi}[Q_\theta (s_t, a_t) - \log \pi_\phi (a_t | s_t)])^2]
$$

The approximation of the derivative of the above objective to update the parameters of the V function:

$$
\hat{\nabla}_{\psi}J_V(\psi) = \nabla_\psi V_{\psi}(s_t) (V_{\psi}(s_t) - Q_\theta (s_t, a_t) + \log \pi_{\phi}(a_t | s_t))
$$

### We train the Q network by minimizing the following error:

$$
J_Q(\theta) = \mathbb{E}_{(s_t, a_t ) \sim \mathcal{D}}\left[ \frac{1}{2}\left(Q_\theta (s_t, a_t) - \hat{Q}(s_t, a_t)\right)^2 \right]
$$

where 

$$
\hat{Q}(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim p} [V_{\bar{\psi}}(s_{t+1})]
$$

For all s-a pairs in the experience replay buffer, we want to minimize the squared difference between the prediction of our Q function and the immediate (one time-step) reward plus teh discounted expected Value of the next state. $V_{\bar{\psi}}$ is the target value function. 

We use the below approximation of the derivative of the above objective is to update the parameters of the Q function:

$$
\hat{\nabla}_{\theta} J_Q (\theta) = \nabla_{\theta}Q_{\theta}(a_t, s_t) (Q_\theta (s_t, a_t) - r(s_t, a_t) - \gamma V_{\bar{\psi}}(s_{t+1}))
$$

### We train the policy network $\pi$ by minimizing the following error

$$
J_{\pi}(\phi) = \mathbb{E}_{s_t \sim \mathcal{D}} \left[D_{KL}\left(\pi(\cdot | s_t) || \frac{\exp (Q_\theta (s_t, \cdot))}{Z_\theta (s_t)}\right)\right]
$$

In order to minimize this objective, the authors use something called the reparameterization trick. This trick is used to make sure that sampling from the policy is a differentiable process so that there are no problems in backpropagating the errors. The policy is now parameterized as follows:

$$
a_t = f_{\phi} (\epsilon_t ; s_t)
$$

The epsilon term is a noise vector sampled from a Gaussian distribution. Now we can express the objective function as follows:

$$
J_{\pi} (\phi) = \mathbb{E}_{s_t \sim \mathcal{D}, \epsilon_t \sim \mathcal{N}}[\log \pi_{\phi}(f_{\phi}(\epsilon_t ; s_t) | s_t) - Q_{\theta}(s_t , f_{\phi} (\epsilon_t ; s_t))]
$$

The normalizing function $Z$ is dropped since it does not depend on the parameter $\phi$. An unbiased estimator for the gradient of the above objective is given as follows:

$$
\begin{array}{ll}
\hat{\nabla}_{\phi} J_{\pi}  (\phi)  &= \nabla_{\phi} \log \pi_{\phi}(a_t | s_t) \\
& + (\nabla_{a_t} \log \pi_{\phi} (a_t | s_t) - \nabla_{a_t} Q(s_t, a_t))\nabla_{\phi} f_{\phi}(\epsilon_t ; s_t) \\
\end{array}
$$

# References

<a id="1">[1]</a>
Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conf. on Uncertainty in Artificial Intelligence, 2016.

<a id="2">[2]</a>
Schulman, J., Abbeel, P., and Chen, X.  Equivalence be-tween policy gradients and soft Q-learning.arXiv preprintarXiv:1704.06440, 2017a.

<a id="3">[3]</a>
Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370â€“2378, 2016

<a id="4">[4]</a>
https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665