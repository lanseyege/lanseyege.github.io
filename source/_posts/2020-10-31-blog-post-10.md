---
useMath: true
title: 'Maximum Margin Methods'
mathjax: true
date: 2020-10-31
permalink: /posts/2020/10/blog-post-10/
tags:
  - Maximum Margin
  - SVM
  - KKT Condition
---
#  Maximum Margin Methods

<!-- more -->


## Karush-Kuhn-Tucker (KKT) Conditions
KKT conditions first-order necessary conditions for a solution in nonlinear programming to be optimal, provided that some regularity conditions are satisfied. ***Inequality Constrained Optimization***, general Problem:

$$
\begin{split}
\max & \quad f(x) \\
\textbf{s.t.:} & \quad g_i(x) \leq 0 \quad \forall i \in l \\
& \quad h_j(x) = 0 \quad \forall j \in J \\
\end{split}
$$

The Lagrangian:

$$
L(x, \lambda, \mu) = f(x) + \sum_{i=1}^m \mu_i(0-g_i(x)) + \sum_{j=1}^k \lambda_j(0 - h_j(x))
$$

The KKT conditions are 

$$
\begin{split}
\frac{\partial}{\partial x} L(x, \lambda \mu) = \nabla f(x^\star) - \sum_i \mu_i \nabla g_i (x^\star) - \sum_j \lambda_j \nabla h_j(x^\star) &= 0 \\
\mu_i g_i(x^\star) &= 0 \quad \forall i \in I \\
\mu_i &\geq 0 \quad \forall i \in I \\
\end{split}
$$

## Support Vector Machine (SVM)
### Separable case 
Consider an input space $\mathcal{X} \in \mathbb{R}^N$ with $N \geq 1$, the target space $$\mathcal{Y} = \left\{-1, +1\right\}$$, and let $f : \mathcal{X} \rightarrow \mathcal{Y}$ be the target function. 

***Geometric Margin*** The geometric margin $\rho_h(\mathbf{x})$ of a linear classifier $h: \mathbf{x} \rightarrow \mathbf{w}\cdot \mathbf{x} + b$ at a point $\mathbf{x}$ is its Euclidean distance to the hyperplane $\mathbf{w}\cdot \mathbf{x} + b = 0$: 

$$
\rho_h(x) = \frac{\lvert \mathbf{w}\cdot \mathbf{x} + b \rvert}{\left\| \mathbf{w} \right\|_2}
$$

The **geometric margin** $\rho_h$ of a linear classifier $h$ for a sample $S = (\mathbf{x}_1, \cdots, \mathbf{x}_m)$ is the minimum geometric margin over the points in the sample, $\rho_h = \min_{i \in [m]}\rho_h(x_i)$, that is the distance of the hyperplane defining $h$ to the closest sample points. 

***Primal optimization problem*** By definition of the geometric margin, the maximum margin $\rho$ of a separating hyperplane is given by 

$$
\begin{equation}
\begin{split}
\rho &= \max_{\mathbf{w},b: y_i (\mathbf{w}\cdot \mathbf{x}_i + b) \geq 0} \min_{i\in [m]} \frac{\lvert \mathbf{w}\cdot \mathbf{x}_i + b \rvert}{\left\| \mathbf{w} \right\|} \\ 
&= \max _{\mathbf{w},b} \min_{i \in [m]} \frac{y_i (\mathbf{w} \cdot \mathbf{x}_i + b)}{\left\| \mathbf{w} \right\|}
\end{split}
\end{equation}
$$

The second equality follows from the fact that, since the sample is linearly separable, for the maximizing pair $(\mathbf{w}, b), y_i(\mathbf{w} \cdot \mathbf{x}_i + b)$ must be non-negative for all $ i \in [m]$. Now, observe that the last expression is invariant to multiplication of $(\mathbf{w}, b)$ by a positive scalar. Thus, we can restrict ourselves to pairs $(\mathbf{w}, b)$ scaled such that $\min_{i \in [m]} y_i (\mathbf{w}\cdot \mathbf{x}_i + b) = 1$: 

$$
\begin{equation}
\begin{split}
\rho &= \max_{\mathbf{w},b:  \min_{i\in[m]} y_i(\mathbf{w}\cdot \mathbf{x}_i + b)=1}\frac{1}{\left\| \mathbf{w} \right\|} = \max_{\mathbf{w},b: \forall i\in[m], y_i(\mathbf{w}\cdot \mathbf{x}_i + b)=1}\frac{1}{\left\| \mathbf{w} \right\|} \\
\end{split}
\end{equation}
$$

The second equality results from the fact that for the maximizing pair $(\mathbf{w}, b)$, the minimum of $y_i (\mathbf{w} \cdot \mathbf{x}_i + b) $ is 1. 

Since maximizing $1 / \left\| \mathbf{w} \right\|$ is equivalent to minimzing $\frac{1}{2}\left\| \mathbf{w} \right\| ^2$, the pair $(\mathbf{w}, b)$ returned by SVM in the separable case is the solution of the following convex optimization problem:

$$
\begin{equation}
\begin{split}
\min_{\mathbf{w},b} & \frac{1}{2} \left\| \mathbf{w} \right\| ^2 \\
s.t.: \quad & y_i (\mathbf{w} \cdot \mathbf{x}_i + b) \geq 1, \forall i \in [m] 
\end{split}
\end{equation}
$$

***The Lagrangian*** 

$$
\begin{equation}
\mathcal{L}(\mathbf{w}, b, \mathbf{\alpha}) = \frac{1}{2}\left\| \mathbf{w} \right\|^2 - \sum_{i=1}^m \alpha_i [y_i(\mathbf{w}\cdot \mathbf{x}_i + b) - 1]
\end{equation}
$$

The KKT conditions are obtained by setting the gradient of the Lagrangian with respect to the primal variables $\mathbf{w}$ and $b$ to zero and by writing the complementarity conditions:



$$
\begin{equation}
\begin{array}{lll}
\nabla_{\mathbf{w}} \mathcal{L}=\mathbf{w}-\sum_{i=1}^{m} \alpha_{i} y_{i} \mathbf{x}_{i}=0 & \Longrightarrow & \mathbf{w}=\sum_{i=1}^{m} \alpha_{i} y_{i} \mathbf{x}_{i} \\
\nabla_{b} \mathcal{L}=-\sum_{i=1}^{m} \alpha_{i} y_{i}=0 & \Longrightarrow & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
\forall i, \alpha_{i}\left[y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right)-1\right]=0 & \Longrightarrow & \alpha_{i}=0 \vee y_{i}\left(\mathbf{w} \cdot \mathbf{x}_{i}+b\right)=1
\end{array}
\end{equation}
$$

and its **dual optimization problem** is: 

$$
\begin{split}
& \max_{\alpha} \sum_{i=1}^m \alpha_i - \frac{1}{2} \sum_{i,j=1}^m \alpha_i \alpha_j y_i y_j (x_i \cdot x_j) \\
\textbf{s.t.:} \quad & \alpha_i \geq 0 \wedge \sum_{i=1}^m \alpha_i y_i = 0, \forall i \in [m] \\ 
\end{split}
$$

### Non-separable Case
For any hyperplane $w \cdot x + b = 0$, there exists $x_i \in S$ such that 

$$
\begin{equation}
y_i [\mathbf{w}\cdot \mathbf{x}_i + b] \ngeq 1 
\end{equation}
$$

While, a relaxed version of these constraints can indeed hold, that is, for each $i \in [m]$, there exist $\xi_i \geq 0$ such that $y_i [\mathbf{w}\cdot \mathbf{x}_i + b] \ngeq 1 - \xi_i$. So the primal optimization problem becomes:

$$
\begin{split}
\min_{w, b, \xi} & \frac{1}{2}\left\|\mathbf{w}\right\|^2 + C \sum_{i=1}^m \xi_i^p \\
\textbf{s.t.}\quad & y_i(\mathbf{w}\cdot \xi_i + b) \geq 1-\xi_i \wedge \xi_i \geq 0, i \in [m] \\
\end{split}
$$

Then the Lagrangian is defined as:

$$
\begin{equation*}
\mathcal{L}(\mathbf{w}, b, \mathbf{\xi}, \mathbf{\alpha}, \mathbf{\beta}) = \frac{1}{2}\left\| \mathbf{w} \right\|^2 + C\sum_{i=1}^m \xi_i - \sum_{i=1}^m \alpha_i [y_i(\mathbf{w}\cdot \mathbf{x}_i + b) - 1 + \xi_i] - \sum_{i=1}^m \beta_i \xi_i 
\end{equation*}
$$



# References
<a id="1">[1]</a> 
@misc{ wiki:xxx,
    author = "{Wikipedia contributors}",
    title = "Karush–Kuhn–Tucker conditions --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions&oldid=983546575",
    note = "[Online; accessed 31-October-2020]"
}

<a id="2">[2]</a>
@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018}
}
