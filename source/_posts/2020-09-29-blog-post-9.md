---
useMath: true
title: 'A Brief Summary on Inverse Reinforcement Learning'
date: 2020-09-29
mathjax: true
permalink: /posts/2020/09/blog-post-9/
tags:
  - Maximum Entropy
  - Inverse RL
  - Causal
---
#  A Brief Summary on Inverse Reinforcement Learning

<!-- more -->

## Inverse Reinforcement Learning
In Inverse Reinforcement Learning (IRL) setting, the reward function is not given explictly but the expert demonstrations are exposed to us. IRL seeks to recover a reward function from (near-)expert demonstration. In [[3]](#3), the task of learning from an expert is called apprenticeship learning (also leaning by watching, imitation leaning, or learning from demonstration). 

## Algorithms for Inverse Reinforcement Learning
[[3]](#3) propose a linear programming method to solve the problem by formulating origin question as a linearly, constrained optimization problem. 

***Theory*** Let a finite state space $S$, a set of actions $$A = \left\{a_1, \cdots, a_k\right\}$$, transition probability matrices $$\left\{ P_a \right\}$$, and a discount factor $\gamma \in (0, 1)$ be given. Then the policy $\pi$ given by $\pi(s)=a_1$ is optimal if and only if, for all $a = a_2, \cdots, a_k$, the reward $R$ satisfies:

$$
(P_{a_1} - P_a) (I - \gamma P_{a_1})^{-1}R \succeq 0 
$$

It's easy to see that the condition $(P_{a_1} - P_a) (I - \gamma P_{a_1})^{-1}R \succ 0$ is necessary and sufficient for $\pi = a_1$ to be the unique optimal policy. It represents the gap of expected value on each state between acting optimally and acting suboptimally. 

The ***key idea*** in this paper is to find a reward function $R(s)$ such that it maximize the gap between expert policy and any other policies. To do so, on each state, we want to maximize the gap of expected value of acting optimally and the ***best*** expected value of taking suboptimal actions. In other words, we seek to maximize the sum of the differences between the quality of the optimal action and the quality of the ***next-best*** action:

$$
\sum_{s \in S}(Q^\pi(s, a_1) - \max_{a\in A\setminus a_1}Q^\pi (s, a))
$$

The objective is:

$$
\max \sum_{i=1}^{N} \min_{a\in A\setminus a^\star} \left\{ P_{a^\star}(i) - P_a(i) \right\}(I - \gamma P_{a^\star})^{-1}R
$$

For large state space, the linear programming formulation is 

$$
\begin{split}
\max &\sum_{s \in S_0} \min_{a \in \left\{a_2, \cdots, a_k\right\}}\left\{p(E_{s^\prime \sim P_{sa_1}}[V^\pi (s^\prime)] - E_{s^\prime \sim P_{sa}}[V^\pi (s^\prime)] )\right\} \\
\textbf{s.t. } &|\alpha_i| \leq 1, i = 1, \cdots, d \\
\end{split}
$$

. This will be solved by Monte-Carlo simulation and LP method. 

## Apprenticeship Learning via Inverse Reinforcement Learning
Feature expectations is expressed as 

$$
\mu(\pi) = E[\sum_{t=0}^\infty \gamma^t \phi(s_t)|\pi] \in R^k
$$

Using this notation, the value of a policy may be written $E_{s_0 \sim D}[V^\pi (s_0)] = w \cdot \mu(\pi)$. Given the trajectories generated by the expert, we denote the empirical estimate for $\mu_E$ by 

$$
\hat{\mu}_E = \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{\infty} \gamma^t \phi(s_t^{(i)})
$$

The ***problem*** is the following: we seeks to find a policy whose performance is close to that of the expert's, on the unknown reward function $R^\star = {w^{\star}}^\top \phi$. To accomplish this, we will find a policy $\tilde{\pi}$ such that $\left\|\mu(\tilde{\pi}) - \mu_E \right\|_2 \leq \epsilon$. 

$$
\begin{split}
\max_{t,w} & \quad t \\
\textbf{s.t.} & w^\top \mu_E \geq w^\top \mu^{(j)} + t, j = 0, \cdots, i-1 \\
& ||w||_2 \leq 1 \\
\end{split}
$$

One important step of the algorithm is: $$t^{(i)} = \max_{w:\left\| w \right\|_{2} \leq 1} \min_{j \in \left\{0, \cdots, i-1 \right\}} w^\top (\mu_E - \mu^{j})$$. 

This is equivalent to finding the maximum margin hyperplane separating two sets of points. So, an SVM solver can be used to find $w^{(i)}$. A simpler algorithm is ***projection method***, as descripted in paper. 



## Max Margin Planning and Learn to Search 
[[4]](#4) propose maximum margin planning method to do imitation learning. It frames the IL problem as a maximum margin structured prediction problem over a space of policies. 

The input to the algorithm is a set of training instances $\mathcal{D} = \left\{ (\mathcal{X}_i, \mathcal{A}_i, p_i, F_i, y_i, \mathcal{L}_i) \right\}_{i=1}^{n}$. 
  - $\mathcal{X}$: State space
  - $\mathcal{A}$: Action space
  - $p_i$: transition probabilities
  - $F_i$: d-dimensional feature vectors in the form of $d\times \lvert\mathcal{X}\rvert \lvert\mathcal{A}\rvert$
  - $y_i$: the desired trajectory
  - $$\mathcal{L}_i : \mathcal{Y}\times\mathcal{Y}\rightarrow R_{+}$$, here, $\mathcal{L}(y, y_i)=\mathcal{L}_i(y) = l_i^\top \mu$. 
    - Intuitively, a loss vector is placed over state-action pairs that defines for each state-action pair how much the learner pays for failing to match the behavior of an example policy $y_i$ as the cumulative loss of the learned policy through this MDP. 
  
Maximum Margin Planning constrained optimization formulation:

$$
\begin{split}
\min_{w, \zeta_i, v_i} \frac{1}{2}\left\|w\right\|^2 + \frac{\gamma}{n}\sum_i\beta_i\zeta_i^q  \\
\text{s.t.} \quad \forall i w^\top F_i\mu_i + \zeta_i \geq s_i^T v_i \\
\forall i, x, a \quad v_i^x \geq (w^\top F_i+ l_i)^{x,a} + \sum_{x^\prime} p_i(x^\prime | x, a )v_i^{x^\prime} \\
\end{split}
$$

In fact, inverse reinforcement learning is an ill-defined problem: there are many optimal policies that can explain a set of demonstrations, and many rewards that can explain an optimal policy. Max-Ent IRL is to handles the former ambiguity.  


##  Maximum Entropy Principle
Entropy is an old concept in physicals that is used to describe the randomness in environments. The greater the entropy, the more random the actions the policy gives. The discrete form of entropy is: 

$$
\begin{equation}
H(X) = \mathbb{E}_X [I(x)] = - \sum_{x \in X} p(x) \log p(x) 
\end{equation}
$$

Similarity, the entropy term for policy has this form:

\begin{equation}
H(\pi(\cdot | s_t)) = - \sum_{a\in A} \pi(a|s_t) \log \pi(a|s_t)
\end{equation}

The entropy term of policy can help the policy to increase the expoloration ability, by adding more possibilities to some rare actions. That could avoid the agent get stuck into local optimum and attain global optimum, to some extend. The idea of learning such maximum entropy model has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics [[1]](#1). The **principle of maximum entropy** states that the probability distribution with the highest entropy, is the one that best represents the current state of knowledge in the context of precisely stated prior data [[2]](#2). 

## Apprenticeship Learning Using Linear Programming
[[5]](#5) One contribution, if one uses the linear programming approach for finding an MDP's optimal policy as a subroutine, then one can modify [[6]](#6) algorithm so that it outputs a stationary policy instead of a mixed policy. Second contribution is the formulation of the apprenticeship learning problem as a linear program. 

We say a policy $\pi$ is $\epsilon$-optimal if $V(\pi^star) - V(\pi) \leq \epsilon$. A policy $\pi$ has occupancy measure $x^\pi$ if 

$$
x_{s,a}^\pi = E[\sum_{t=0}^\infty \gamma^t \mathbb{1}_{(s_t=s \wedge a_t =a)}\vert \alpha, \pi, \theta]
$$

for all $s,a$. The goal of apprenticeship learning is to find an apprentice policy $\pi^A$ such that $V(\pi^A) \geq V(\pi^E)$. 

## A Game-Theoretic Approach to Apprenticeship Learning
[[6]](#6) propose a method to produce a policy that may substantially better than the expert's based on a game-theoretic view of the problem. They pose the problem as learning to play a two-player zero-sum game in which the apprentice chooses a policy, and the environment chooses a reward function. The goal of the apprentice is to maximize performance relative to the expert, even though the reward function may be adversarially selected by the environment with respect to this goal. 

### References
<a id="1">[1]</a> 
@article{haarnoja2017reinforcement,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017}
} 


<a id="2">[2]</a>
https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df

<a id="3">[3]</a>
@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

<a id="4">[4]</a>
@inproceedings{ratliff2006maximum,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006}
}

<a id="5">[5]</a>
@inproceedings{syed2008apprenticeship,
  title={Apprenticeship learning using linear programming},
  author={Syed, Umar and Bowling, Michael and Schapire, Robert E},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={1032--1039},
  year={2008}
}

<a id="6">[6]</a>
@inproceedings{syed2008game,
  title={A game-theoretic approach to apprenticeship learning},
  author={Syed, Umar and Schapire, Robert E},
  booktitle={Advances in neural information processing systems},
  pages={1449--1456},
  year={2008}
}
