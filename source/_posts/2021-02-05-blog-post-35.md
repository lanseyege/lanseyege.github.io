---
useMath: true
title: ' Note on "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting"'
mathjax: true
date: 2021-02-05
permalink: /posts/2021/02/blog-post-35/
tags:
  - Informer
  - Transformer
  - Time-Series
  - Forecasting
---

# Informer

<!-- more -->

(The formulation of this paper is not so clear.)

## Problem Description

The input $\mathcal{X}^t = \left\{ x_1^t, \ldots, x_{L_x}^t | x+i^t \in \mathbb{R}^{d_x} \right\}$ at time $t$, and output is to predict corresponding sequence $\mathcal{Y}^t = \left\{ y_1^t, \ldots, y_{L_y}^t | y_i^t \in \mathbb{R}^{d_y} \right\}$. 

## Solution abstraction

Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model. Transformer can do this job but have following drawbacks:

  - The quadratic computation of self-attention.
  - The memory bottleneck in stacking layers for long inputs.
  - The speed plunge in predicting long outputs.

The paper proposes Informer to address these issues.




## Methodology

### Efficient Self-attention Mechanism

$$
\mathcal{A}(Q, K, V) = \operatorname{Softmax}(\frac{\bar{Q}K^\top}{\sqrt{d}}) V 
$$

where $\bar{Q}$ is a sparse matrix of the same size of $q$. 

### Encoder

$$
X_{j+1}^t = \operatorname{MaxPool}(\operatorname{ELU}(\operatorname{Conv1d}([X_j^t]_{AB})))
$$

where $[\cdot]_{AB}$ contains the Multi-head probSparse self-attention and essential operations in attention block. 

### Decoder

$$
X_{\text{feed\_de}} ^t = \operatorname{Concat}(X_{\text{token}}^t, X_0^t) \in \mathbb{R}^{(L_{\text{token}} + L_y) \times d_{\text{model}} }
$$



# References

<a id="1">[1]</a> 
@article{zhou2020informer,
  title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},
  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},
  journal={arXiv preprint arXiv:2012.07436},
  year={2020}
}


<a id="2">[2]</a> 


