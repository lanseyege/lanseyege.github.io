---
useMath: true
title: 'notes on "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction"'
mathjax: true
date: 2021-05-237
permalink: /posts/2021/05/blog-post-47/
tags:
  - rate reduction
  - deep learning
---

# ReduNet [[1]](#1)

<!-- more -->

This work attempts to provide a plausible theoretical framework that aims to interpret modern deep (convolutional) networks from the principles of data compression and discriminative representation. 

**Motivation** The design of deep networks are often based on years of trial and error, then trained via back propagation, and then deployed as a "black box". It lacks of rigorous mathematical principles, modeling and analysis. It naturally raises a fundamental question that we aim to address in this paper: ***how to develop a principled mathematical framework for better understanding and design of deep networks?***

**A new theoretical framework**  The paper develop a new theoretical framework for understanding deep networks around the following two questions:

  - Objective of Representation Learning : What intrinsic structures of the data should we learn, and how should we represent such structures? What is a principled objective function for learning a good representation of such structures, instead of choosing heuristically or arbitrarily?
  - Architecture of Deep Networks : Can we justify the structures of modern deep networks from such a principle? In particular, can the networks' layered architecture and operators (linear or nonlinear) all be derived from this objective, rather than designed heuristically and evaluated empirically?

The paper's answer to the two questions are:

  - A principled objective for a deep network is to learn a low-dimensional linear discriminative representation of the data. The optimality of such a representation can be evaluated by a principled measure from (lossy) data compression, known as ***rate reduction***. 
  - Deep networks can be naturally interpreted as ***optimization schemes for maximizing this measure***. 

Not only does this framework offer new perspectives to understand and interpret modern deep networks, they also provide new insights that can potentially change and improve the practice of deep networks. 

## The Principle of Maximal Coding Rate Reduction 

Whether the given data $\mathbf{X}$ of a mixed distribution $\mathcal{D} = \{\mathcal{D}^j\}_{j=1}^k$ can be effectively classified depends on how separable (or discriminative) the component distribution $\mathcal{D}^j$ are. One popular working ***assumption*** is that the distribution of each class has relatively low-dimensional intrinsic structures. 

We ***assume*** the distribution $\mathcal{D}^j$ of each class has a support on a low-dimensional submanifold, say $\mathcal{M}^j$ with dimension $d_j \ll D$, and the distribution $\mathcal{D}$ of $x$ is supported on the mixture of those submanifolds, $\mathcal{M} = \cup_{j=1}^k \mathcal{M}^j$, in the high-dimensional ambient space $\mathbb{R}^D$. 

With the manifold assumption, we want to learn a mapping $z = f(x, \theta)$ that maps each of the submanifolds $\mathcal{M}^j \subset \mathbb{R}^D$ to a linear subspace $\mathbf{S}^j \subset \mathbb{R}^n$. We require the learned representation to have the following properties, called a linear discriminative representation (LDR):

  - Within-Class Compressible
  - Between-Class Discriminative
  - Diverse Representation

In this work, to learn a discriminative linear representation for intrinsic low-dimensional structures from high-dimensional data, they propose an information-theoretic measure that maximizes the coding rate difference between the whole dataset and the sum of each individual class, known as ***rate reduction***.

### Measure of Compactness for Linear Representation 

***Rate distortion*** measures the "compactness" of a random distribution: Given a random variable $z$ and a prescribed precision $\epsilon > 0$, the rate distortion $R(z, \epsilon)$ is the minimal number of binary bits needed to encode $z$ such that the expected decoding error is less than $\epsilon$, i.e., the decoded $\hat{z}$ satisfied $\mathbb{E}[\| z - \hat{z} \| _2] \leq \epsilon$. 

***Rate distortion for finite samples on a subspace.*** The compactness of learned features as a whole can be measured in terms of the average coding length per sample (as the sample size m is large), a.k.a. the coding rate subject to the distortion $\epsilon$:

$$
\begin{equation}
R(\mathbf{Z}, \epsilon) = \frac{1}{2} \log \det (\mathbf{I} + \frac{n}{m \epsilon^2} \mathbf{Z}\mathbf{Z}^\star)
\end{equation}
$$

***Rate distortion of samples on a mixture of subspaces.*** We may partition the data (representation) $\mathbf{Z}$ into multiple subsets: $\mathbf{Z} = \mathbf{Z}^1 \cup \mathbf{Z}^2 \ldots \mathbf{Z}^k$ with each $\mathbf{Z}^j$ containing samples in one low-dimensional subspace. Let $\Pi = \{ \Pi ^ j \in \mathbb{R}^{m \times m} \}_{j=1}^k$ be a set of diagonal matrices whose diagonal entries encode the membership of the $m$ samples in the $k$ classes. The average number of bits per sample (the coding rate) is 

$$
\begin{equation}
R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi}) = \sum_{j = 1} ^k \frac{tr(\mathbf{\Pi})}{2m} \log \det \left( \mathbf{I} + \frac{d}{tr(\mathbf{\Pi}^j)\epsilon ^2} \mathbf{Z}\mathbf{\Pi}^j \mathbf{Z}^\star  \right)
\end{equation}
$$

When $\mathbf{Z}$ is given, $R_c (\mathbf{Z}, \epsilon | \mathbf{\Pi})$ is a concave function of $\mathbf{\Pi}$. 




### Principle of Maximal Coding Rate Reduction 

The learned features should follow the basic rule that ***similarity contracts and dissimilarity contrasts***. To be more precise, a good (linear) discriminative representation $\mathbf{Z}$ of $\mathbf{X}$ is one such that, given a partition $\mathbf{\Pi}$ of $\mathbf{Z}$, achieves a large difference between the coding rate for the whole and that for all the subsets:

$$
\begin{equation}
\Delta R(\mathbf{Z}, \mathbf{\Pi}, \epsilon) = R(\mathbf{Z}, \epsilon) - R_c(\mathbf{Z}, \epsilon | \mathbf{\Pi})
\end{equation}
$$

If we choose our feature mapping to be $z = f(x, \theta)$ (a dnn), the overall process of the feature representation and the resulting rate reduction w.r.t. certain partition $\mathbf{\Pi}$ can be illustrated by the following diagram:

$$
\begin{equation}
\boldsymbol{X} \stackrel{f(\boldsymbol{x}, \boldsymbol{\theta})}{\longrightarrow} \boldsymbol{Z}(\boldsymbol{\theta}) \stackrel{\Pi, \epsilon}{\longrightarrow} \Delta R(\boldsymbol{Z}(\boldsymbol{\theta}), \boldsymbol{\Pi}, \epsilon)
\end{equation}
$$

**Normalization**. To make the amount of reduction comparable between different representations, we need to normalize the scale of the learned features, either by imposing the Frobenius norm of each class $\mathbf{Z}^j$ to scale with the number of features in $\mathbf{Z}^j \in \mathbb{R}^{n \times m_j} : \| \mathbf{Z}^j \| _F^2 = m_j$ or by normalizing each feature to be on the unit sphere: $z ^i \in \mathcal{S}^{n-1}$. 

Once the representations can be compared fairly, our goal becomes to learn a set of features $\mathbf{Z}(\theta) = f(\mathbf{X}, \theta)$ and their partition $\mathbf{\Pi}$ such that they maximize the reduction between the coding  rate of all features and that of the sum of features w.r.t. their classes:

$$
\begin{equation}
\begin{split}
\max_{\theta, \Pi} \Delta R(\mathbf{Z}(\mathbf{\theta}), \mathbf{\Pi}, \epsilon) &= R(\mathbf{Z}(\mathbf{\theta}), \epsilon) - R_c (\mathbf{Z}(\theta), \epsilon | \mathbf{\Pi}),  \\
 \text{ s.t. } & \| \mathbf{Z}^j (\mathbf{\theta}) \|_F ^2 = m_j , \mathbf{\Pi} \in \Omega
\end{split}
\end{equation}
$$

This is refered to as the principle of maximal coding rate reduction (MC$R^2$). 

### Properties of the Rate Reduction Function 

The MC$R^2$ principle is very general and can be applied to representations $\mathbf{Z}$ of any distributions with any attributes $\mathbf{\Pi}$ as long as the rates $R$ and $R_c$ for the distributions can be accurately and efficiently evaluated. 

The optimal representation $\mathbf{Z_\star}$ has the following desired properties:

**Theorem 1** (Informal Statement) Suppose $\mathbf{Z_\star} = \mathbf{Z_\star^1} \cup \cdots \cup \mathbf{Z_\star^k}$ is the optimal solution that maximizes the rate reduction with the rates $R$ and $R_c$. Assume that the optimal solution satisfies $rank(\mathbf{Z_\star^j}) \leq d_j$. We have:
  
  - Between-class Discriminative: As long as the ambient space is adequately large $(n \geq \sum_{j=1}^k d_j)$, the subspaces are all orthogonal to each other, i.e. $(\mathbf{Z_\star^i}) * \mathbf{Z_\star^j} = 0$ for $i \neq j.$
  - Maximally Diverse Representation: As long as the coding precision is adequately high, i.e., $\epsilon^4 < \min_j \{\frac{m_j}{m} \frac{n^2}{d_j^2} \}$, each subspace achieves its maximal dimension, i.e. $rank(\mathbf{Z_\star^j}) = d_j$. In addition, the largest $d_j - 1$ singular values of $\mathbf{Z_\star}^j$ are equal. 




# References

<a id="1">[1]</a> 
@article{chan2020redunet,
  title={ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction},
  author={Chan, Kwan Ho Ryan and Yu, Yaodong and You, Chong and Qi, Haozhi and Wright, John and Ma, Yi},
  journal={arXiv preprint arXiv:2105.10446},
  year={2021}
}

<a id="2">[2]</a> 


