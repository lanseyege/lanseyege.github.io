---
useMath: true
title: 'Stein Variational Gradient Decsent: Stein Method'
mathjax: true
date: 2020-08-26
permalink: /posts/2020/08/blog-post-6/
tags:
  - Stein
  - SVGD
---

# Stein's Method

<!-- more -->

## Stein's Method

Stein's method is a technique that can quantify the error in the approximation of one distribution by another in a variety of metrics. Stein's method is a general theoretical tool for obtaining bounds on distances between distributions. 

## Foundation of Stein's Method

(The following content is almost directly copied form [[4]](#4))

Let $Z$ be a random variable on $\mathbb{R}$ with differentiable probability density $p$. Our main emphasis here is for $p$ the standard normal density, but this argument works more generally. Now let $\mathcal{F}$ be the set of differentiable real functions $f$ such that $f(x)p(x) \rightarrow 0$ as $\lvert x \rvert \rightarrow \infty$. Then clearly

$$
\begin{split}
0 &= \int_{-\infty}^\infty \frac{d}{dx}\left\{ f(x)p(x)\right\} dx  \\
 &= \int_{-\infty}^{\infty} \left\{ f^\prime(x) + \phi(x)f(x) \right\}p(x) dx \\
\end{split}
$$

with $\phi(x) := p^\prime(x) / p(x)$: for $p$ the standard normal density, $\phi(x) = -x$. Hence any function $h$ of the form $f^\prime + \phi f$ with $f \in \mathcal{F}$ automatically has $\mathbb{E} h(Z) = 0$. Conversely, given any continuous $h$ with $\mathbb{E}\lvert h(Z) \rvert < \infty$, the function $h - \mathbb{E}h(Z)$ can be written in the form $f^\prime + \phi f$ with $f\in \mathcal{F}$. (Remark: is this Stein's Lemma??)

## One example:

**example: begin** If $g = f^\prime + \phi f$, then 

$$
\begin{equation}
\int_{-\infty}^{X} g(x) p(x) dx = \int_{-\infty}^{X} \frac{d}{dx} \left\{ f(x)p(x)\right\}dx = f(X)p(X)
\end{equation}
$$

and that this argument can be reversed: $f$ so defined, for an arbitrary $g$ for which $\mathbb{E}\lvert g(Z) \rvert < \infty$, is such that $g = f^\prime + \phi f$. Hence we can take $f = f_h$ to be defined by

$$
\begin{split}
f_h(X)p(X) &= \int_{-\infty}^X \left\{ h(x) - \mathbb{E}h(Z) \right\}p(x) dx
&= - \int_X^\infty \left\{ h(x) - \mathbb{E}h(Z) \right\} p(x) dx
\end{split}
$$

noting that it's then directly checked that $f_h \in \mathcal{F}$. This allows us to write 

$$
\begin{equation}
\mathbb{E} h(W) - \mathbb{E}h(Z) = \mathbb{E}\left\{ f_h^\prime(W) + \phi(W)f_h(W) \right\}
\end{equation}
$$

How to get the last equation? By derivating $w$ at both side, 

$$
[f_h(x)p(x)]^\prime \vert_{x = W} = \left\{h(W)-\mathbb{E}h(Z)\right\}_{\vert_{x=W}}p(W)
$$

the left:

$$
\begin{equation}
\begin{split}
\text{left} &:= \int_{-\infty}^W [f_h(x)p(x)]^\prime dx  \\
&= \int f_h^\prime p + \int f_n p^\prime  \\
&= \mathbb{E}f_h^\prime + \int f_h \frac{p^\prime}{p} p dx \\
&= \mathbb{E}f_h^\prime + \mathbb{E}\phi f_h \\
\end{split}
\end{equation}
$$

the right:

$$
\begin{equation}
\begin{split}
\text{right} &:= \int_{-\infty}^W \left\{ h(x) - \mathbb{E}h(Z) \right\} p(x) dx \\
&= \mathbb{E}h(W) - \mathbb{E}h(Z) \\
\end{split}
\end{equation}
$$

for any random variable $W$ for which the expectations exist; in particular, for standard normal approximation,

$$
\begin{equation}
\mathbb{E}h(W) - \mathbb{E}h(Z) = \mathbb{E}\left\{ f_h^\prime (W) - Wf_h(W) \right\}
\end{equation}
$$

**example: end** 

***Important Conclusion!!*** Taking the supremum of the left hand side over test funcitons $h \in \mathcal{H}$ gives a measure of the distance between the distributions of $W$ and $Z$. The distance can in turn be computed by taking the supremum of the right side over $h \in \mathcal{H}$. ..., in many many circumstances, ***expressing the difference $\mathbb{E}h(W) - \mathbb{E}h(Z)$ in the entirely equivalent form $$\mathbb{E}\left\{ f_h^\prime (W) + \phi(W)f_h(W) \right\}$$ makes it easier to bound.***

***Error Bounded*** [[5]](#5) Let $\mathcal{D}$ be the set of all $f : \mathbb{R}\rightarrow \mathbb{R}$ that are twice continuously differentiable, and $\lvert f(x) \rvert \leq 1, \lvert f^\prime(x) \rvert \leq 1$ and $\lvert f^{\prime\prime}(x) \rvert \leq 1$ for all $x \in \mathbb{R}$. Let $Z$ be a standard normal random variable and $W$ be any random variable. Then

$$
\begin{equation}
\sup_{t\in \mathbb{R}} \lvert \mathbb{P}(W \leq t) - \mathbb{P}(Z \leq t) \rvert \leq 2 (\sup_{f\in \mathcal{D}} \lvert \mathbb{E}(f^\prime(W)) - Wf(W)) \rvert )^{1/2} 
\end{equation}
$$

## Stein's Lemma, Identity and Operator

***Stein's Lemma*** Define the functional operator $\mathcal{A}$ by

$$
\begin{equation}
\mathcal{A}f(x) = f^\prime(x) - xf(x)
\end{equation}
$$

  - If $Z$ has the standard normal distribution, then $\mathbb{E}\mathcal{A}f(Z) = 0$ for all absolutely continuous $f$ with $\mathbb{E} \lvert f^\prime(Z)\rvert < \infty$.
  - If for some random variable $W$, $\mathbb{E}\mathcal{A}f(W) = 0$ for all absolutely continuous functions $f$ with $|| f^\prime || < \infty$, then $W$ has the standard normal distribution. 

The operator $\mathcal{A}$ is referred to as a characterizing operator of the standard normal distribution. 

***(Stein) score function*** Assume that $\mathcal{X}$ is a subset of $\mathbb{R}^d$ and $p(x)$ a continuous differentiable (also called smooth) density whose support is $\mathcal{X}$. The (Stein) score function of $p$ is defined as

$$
\begin{equation}
s_p = \nabla_x \log p(x) = \frac{\nabla_x p(x)}{p(x)}
\end{equation}
$$

We say that a function $f : \mathcal{X} \rightarrow \mathbb{R}$ is in the ***Stein class*** of $p$ if $f$ is smooth and satisfies

$$
\begin{equation}
\int_{x\in \mathcal{X}} \nabla_x(f(x)p(x))dx = 0
\end{equation}
$$

It holds with a condition when $\mathcal{X} = \mathbb{R}^d$, if 

$$
\begin{equation}
\lim_{||x|| \rightarrow \infty} f(x)p(x) = 0
\end{equation}
$$

***Stein's Operator*** The Stein's operator of $p$ is a linear operator acting on the Stein class of $p$, defined as

$$
\begin{equation}
\mathcal{A}_p f(x) = s_p(x)f(x) + \nabla_xf(x)
\end{equation}
$$

***Stein's Identity*** Assume $p(x)$ is a smooth density supported on $\mathcal{X}$, then 

$$
\begin{equation}
\mathbb{E}_p[\mathcal{A}_p f(x)] = \mathbb{E}_p[s_p(x) f(x)^\top + \nabla f(x)] = 0
\end{equation}
$$

for any $f$ that is in the Stein class of $p$. 


***Lemma [[6]](#6)*** Assume $p(x)$ and $q(x)$ are smooth densities supported on $\mathcal{X}$ and $f(x)$ is in the Stein class of $p$, we have 

$$
\begin{equation}
\mathbb{E}_p[\mathcal{A}_q f(x)] = \mathbb{E}_p[(s_q(x) - s_p(x))f(x)^\top]
\end{equation}
$$

***Lemma Stein's identity (again..)***  
Roughly speaking, it relies on the basic fact that two smooth densities $p(x)$ and $q(x)$ supported on $\mathcal{R}$ are **identical** if and only if 

\begin{equation}
\mathbb{E}_p [s_q(x)f(x) + \nabla_x f(x)] = 0
\end{equation}

for smooth functions $f(x)$ with proper zero-boudary conditions, where $s_q(x) = \nabla_x \log q(x) = \nabla_x q(x) / q(x)$ is called the (Stein) score function of $q(x)$. The equation is known as **Stein's identity**. 

## Stein Discrepancy Measure and Kernelized Stein Discrepancy (KSD)

***Stein discrepancy measure*** between $p$ and $q$ is 

$$
\begin{equation}
\mathbb{S}(p, q) = \max_{f \in \mathcal{F}} (\mathbb{E}_p[s_q(x)f(x) + \nabla_xf(x)])^2
\end{equation}
$$

where $\mathcal{F}$ is a set of smooth functions that satisfies $$\mathbb{E}_p[s_q(x)f(x) + \nabla_xf(x)]=0$$ and is also rich enough to ensure $\mathbb{S}(p,q)>0$ whenever $p \neq q$. 


***Kernelized Stein Discrepancy (KSD)*** $\mathbb{S}(p, q)$ between distribution $p$ and $q$ is defined as 

$$
\begin{equation}
\mathbb{S}(p, q) = \mathbb{E}_{x, x^\prime \sim p}[\delta_{q,p}(x)^\top k(x, x^\prime)\delta_{q,p}(x^\prime)]
\end{equation}
$$

where $\delta_{q,p}(x) = s_q(x) - s_p(x)$ is the score difference between $p$ and $q$, and $x, x^\prime$ are i.i.d draws from $p(x)$. 

***Proposition*** Define $g_{p,q}(x)=p(x)(s_q(x)-s_p(x))$. Assume $k(x,x^\prime)$ is integrally strictly positive definite, and $p, q$ are continuous densities with $\left\|\boldsymbol{g}_{p, q}\right\|_{2}^{2}<\infty$, we have $\mathbb{S}(p,q) \geq 0$ and $\mathbb{S}(p, q) = 0$ if and only if $$p=q$$. (This establishes $\mathbb{S}(p,q)$ as a valid discrepancy measure.)

***Definition*** A kernel $k(x,x^\prime)$ is said to be in the Stein class of $p$ if $k(x, x^\prime)$ has a continuous second order partial derivatives, and both $k(x, \cdot)$ and $k(\cdot, x)$ are in the Stein class of $p$ for any fixed $x$. 

***Theorem*** Assume $p$ and $q$ are smooth densities and $k(x, x^\prime)$ is in the Stein class of $p$. Define

$$
\begin{equation}
\begin{split}
u_q(x,x^\prime) &= s_q(x)^\top k(x,x^\prime)s_q(x^\prime) + s_q(x)^\top \nabla_{x^\prime}k(x,x^\prime) \\
& \quad \quad + \nabla_x k(x,x^\prime)^\top s_q(x^\prime) + \text{trace}(\nabla_{x,x^\prime}k(x,x^\prime)) \\
\end{split}
\end{equation}
$$

then, $$\mathbb{S}(p,q) = \mathbb{E}_{x,x^\prime \sim p}[u_q(x, x^\prime)]$$

***Theorem*** Assume $k(x, x^\prime)$ is a positive definite kernel in the Stein class of $p$, with positive eigenvalues $\left\{\lambda_j\right\}$ and eigenfunctions $\left\{e_j(x)\right\}$, then $u_q(x, x^\prime)$ is also a positive definite kernel, and can be rewritten into 

$$
\begin{equation}
u_q(x, x^\prime) = \sum_j \lambda_j [\mathcal{A}_q e_j(x)]^\top [\mathcal{A}_q e_j(x^\prime)]
\end{equation}
$$

where $\mathcal{A}_qe_j(x) = s_q(x)e_j(x) + \nabla_xe_j(x)$ is the Stein's operator acted on $e_j$. In addition,

$$
\begin{equation}
\mathbb{S}(p, q) = \sum_j \lambda_j || \mathbb{E}_{x\sim p}[\mathcal{A}_qe_j(x)] ||_2^2 
\end{equation}
$$

Note that although $\left\{e_j\right\}$ are orthonormal, the $\left\{\mathcal{A}_qe_j(x)\right\}$ are no longer othonormal in general. 

***Theorem*** Let $\mathcal{H}$ be the RKHS related to a positive definite kernel $k(x, x^\prime)$ in the Stein class of $p$. Denote by $\beta(x^\prime) = \mathbb{E}_{x\sim p}[\mathcal{A}_q k_{x^\prime}(x)]$, then 

$$
\begin{equation}
\mathbb{S}(p, q) = ||\beta||_{\mathcal{H}^d}^2
\end{equation}
$$

Further, we have $\langle f, \beta \rangle_{\mathcal{H}^d} = \mathbb{E}_x[\text{trace}(\mathcal{A}_qf)]$ for $f \in \mathcal{H}^d$, and hence

$$
\begin{equation}
\sqrt{S(p, q)}=\max _{f \in \mathcal{H}^{d}}\left\{\mathbb{E}_{x}\left[\operatorname{trace}\left(\mathcal{A}_{q} f\right)\right] \text { s.t. }\|f\|_{\mathcal{H}^{d}} \leq 1\right\}
\end{equation}
$$

where the maximum is achieved when $f = \beta / \left\|\beta\right\|_{\mathcal{H}^d}$.

## How to Use KSD? (Goodness-of-fit Testing Based on KSD)

The form in - allows efficient estimation of $\mathbb{S}(p, q)$ in practice. Given i.i.d sample $\left\{ x_i \sim p \right\}$ ($p$ is unknown) and the score function $s_q(x)$, we can estimate $\mathbb{S}(p, q)$ by

$$
\begin{equation}
\hat{\mathbb{S}}_u(p, q) = \frac{1}{n(n-1)}\sum_{1\leq i \neq j \leq n}u_q(x_i, x_j)
\end{equation}
$$

where $\hat{\mathbb{S}}_u(p, q)$ is a form of U-statistics. $\mathbb{S}(p, q)$ can also be estimated using a V-statistics of form $\frac{1}{n^2}\sum_{i,j=1}^n u_q(x_i, x_j)$, which provides a biased estimator. 

# References

<a id="1">[1]</a>
Fox, R., Pakman, A., and Tishby, N. Taming the noise in reinforcement learning via soft updates. In Conf. on Uncertainty in Artificial Intelligence, 2016.

<a id="2">[2]</a>
Schulman, J., Abbeel, P., and Chen, X.  Equivalence be-tween policy gradients and soft Q-learning.arXiv preprintarXiv:1704.06440, 2017a.

<a id="3">[3]</a>
Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016

<a id="4">[4]</a>
Barbour, Andrew D., and Louis HY Chen. "Steins (magic) method." arXiv preprint arXiv:1411.1179 (2014).

<a id="5">[5]</a>
Reinert, Gesine. "A short introduction to Stein’s method." Lecture Notes (2011).

<a id="6">[6]</a>
Ley, Christophe, and Yvik Swan. "Stein's density approach and information inequalities." Electronic Communications in Probability 18 (2013).