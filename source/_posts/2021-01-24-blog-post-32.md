---
useMath: true
title: 'Optimization on Manifold '
mathjax: true
date: 2021-01-24
permalink: /posts/2021/01/blog-post-32/
tags:
  - Manifold
  - Optimization
  - Convergence
---

# Manifold Optimization

Riemannian optimization problem:

$$
\min f(x) \text{ such that } x \in \mathcal{X} \subset \mathcal{M},
$$

<!-- more -->

where $(\mathcal{M}, \varrho)$ denotes a Riemannian manifold with the Riemannian metric $\varrho$, $\mathcal{X} \subset \mathcal{M}$ is a nonempty, compact, geodesically convex set, and $f : \mathcal{X} \rightarrow \mathbb{R}$ is geodesically convex and geodesically $L$-smooth. Here $G$-convex function may be non-convex in the usual Euclidean space but convex along the manifold, and thus can be solved by a global optimization solver. 


## Riemannian Manifold 

A **Riemannian Manifold** $(\mathcal{M}, \varrho)$ is a real smooth manifold $\mathcal{M}$ equipped with a Riemannian metric $\varrho$. Let $\left\langle w_{1}, w_{2}\right\rangle_{x}=\varrho_{x}\left(w_{1}, w_{2}\right)$ denote the inner product of $w_1, w_2 \in T_x \mathcal{M}$; and the norm of $w \in T_x \mathcal{M}$ is defined as $\| w \| _x = \sqrt{\varrho_x (w, w)}$, where the metric $\varrho$ induces an inner product structure in each tangent space $T_x \mathcal{M}$ associated with every $x \in \mathcal{M}$. 

A **geodesic** is a constant speed curve $\gamma : [0, 1] \rightarrow \mathcal{M}$ that is locally distance minimizing. Let $y \in \mathcal{M}$ and $w \in T_x \mathcal{M}$, then an exponential map $y = \text{Exp}_x (w) : T_x \mathcal{M} \rightarrow \mathcal{M}$ maps $w$ to $y$ on $\mathcal{M}$, such that there is a geodesic $\gamma$ with $\gamma(0) = x, \gamma(1) = y$ and $\gamma(0) = w$. If there is a unique geodesic between any two points in $\mathcal{X} \subset \mathcal{M}$, the exponential map has inverse $\text{Exp}^{-1}_x : \mathcal{X} \rightarrow T_x \mathcal{M}$, i.e., $w = \text{Exp}^{-1} _x (y)$, and the geodesic is the unique shortest path with $\| \text{Exp}^{-1}_x (y) \| _x = \| \text{Exp}^{-1}_y (x) \| _y = d(x, y)$, where $d(x, y)$ is the geodesic distance between $x, y \in \mathcal{M}$. 

**Parallel transport** $\Gamma_{x}^{y}: T_{x} \mathcal{M} \rightarrow T_{y} \mathcal{M}$ maps a vector $w \in T_x \mathcal{M}$ to $\Gamma_x ^y w \in T_y \mathcal{M}$, and preserves inner products and norm, that is,  $\left\langle w_{1}, w_{2}\right\rangle_{x}=\left\langle\Gamma_{x}^{y} w_{1}, \Gamma_{x}^{y} w_{2}\right\rangle_{y} \text { and }\left\|w_{1}\right\|_{x}=\left\|\Gamma_{x}^{y} w_{1}\right\|_{y}$, where $w_1, w_2 \in T_x \mathcal{M}$. 


For any $x, y \in \mathcal{X}$ and any geodesic $\gamma$ with $\gamma(0) = x, \gamma(1) = y$ and $\gamma(t) \in \mathcal{X}$ for $t \in [0, 1]$ such that $f(\gamma(t)) \leq (1-t) f(x) + t f(y)$, then $f$ is **geodesically convex (G-convex)**, and an equivalent definition is formulated as follows;

$$
f(y) \geq f(x) + \left\langle\operatorname{grad} f(x), \operatorname{Exp}_{x}^{-1}(y)\right\rangle_{x}, 
$$

where $\operatorname{grad}f(x)$ is the Riemannian gradient of $f$ at $x$. A function $f : \mathcal{X} \rightarrow \mathbb{R}$ is called **geodesically $\mu$-strongly convex ($\mu$-strongly G-convex)** if for any $x, y \in \mathcal{X}$, the following inequality holds

$$
f(y) \geq f(x) + \left\langle \operatorname{grad} f(x) , \operatorname{Exp}_{x}^{-1} (y) \right\rangle_x + \frac{\mu}{2} \| \operatorname{Exp}_{x}^{-1} (y) \|_x ^2
$$

A differential function $f$ is **geodesically $L$-smooth (G-L-smooth)** if its gradient is $L$-Lipschitz, i.e., 

$$
f(y) \leq f(x) + \left\langle \operatorname{grad} f(x), \operatorname{Exp}_x^{-1} (y) \right\rangle _x + \frac{L}{2} \| \operatorname{Exp}_x ^{-1} (y) \| _x ^2
$$

## Global Convergence Rate Analysis

provided the global complexity analysis of first-order algorithms for $G$-convex optimization, and designed the following **Riemannian** gradient descent rule:

$$
x_{k+1} = \text{Exp}_{x_k} (-\eta \text{grad} f(x_k))
$$

where $k$ is the iterate index, $\text{Exp}_{x_k}$ is an exponential map at $x_k$, $\eta$ is a step-size or learning rate, and $\text{grad} f(x_k)$ is the Riemannian gradient of $f$ at $x_k \in \mathcal{X}$.  

# References

<a id="1">[1]</a> 
@inproceedings{liu2017accelerated,
  title={Accelerated First-order Methods for Geodesically Convex Optimization on Riemannian Manifolds.},
  author={Liu, Yuanyuan and Shang, Fanhua and Cheng, James and Cheng, Hong and Jiao, Licheng}
}

<a id="2">[2]</a> 

