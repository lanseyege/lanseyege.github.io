---
useMath: true
title: 'Fictitious Self-Play'
mathjax: true
date: 2021-07-09
permalink: /posts/2021/07/blog-post-52/
tags:
  - Fictitious
  - Self-Play
---

# Fictitious Self-Play

Fictitious play is a popular game-theoretic model of learning in games. This paper aims at ***efficiently computing Nash equilibria of imperfect-information games***.

<!-- more -->

In fictitious play, players repeatedly play a game, at each iteration choosing a best repsonse to their opponents' average strategies. The average strategy profile of fictitious players converges to a **Nash equilibrium** in certain classes of games.

## Background

### Extensive-Form

**Extensive-form games** are a model of sequential interaction involving multiple agents. 

  - $\mathcal{N}=\left\{1,\cdots,n\right\}$ denotes the set of players. 
  - $\mathcal{S}$ is a set of **states** corresponding to nodes in a finite rooted game tree. 
  - For each state node $s \in \mathcal{S}$ the edges to its successor states define a set of **actions** $\mathcal{A}(s)$ available to a player or chance in state $s$. 
  - The player function $P : \mathcal{S} \rightarrow \mathcal{N} \cup \left\{c\right\}$, with $c$ denoting chance, determines who is to act at a given state.
  - **Chance** is considered to be a particular player that follows a fixed randomized strategy that determins the distribution of chance events at chance nodes. 
  - For each player $i$ there is a corresponding set of **information states** $\mathcal{U}^i$ and an information function $I^i : \mathcal{S} \rightarrow \mathcal{U}^i$ that determines which states are indistinguishable for the player by mapping them on the same information state $u \in \mathcal{U}^i$.
  - We assume games with **perfect recall**
  - $R : \mathcal{S} \rightarrow \mathbb{R}^n$ maps terminal states to a vector whose components correspond to each player's payoff. 
  - A player's **behavioural strategy**, $\pi^i(u) \in \Delta (\mathcal{A}(u)), \forall u \in \mathcal{U}^i$, determines a probability distribution over actions given an information state, and $\Delta_b^i$ is the set of all behavioural strategies of player $i$. 
  - A **strategy profile** $\pi = (\pi^1 , \ldots, \pi^n)$ is a collection of strategies for all players. 
  - $\pi^{-i}$ refers to all strategies in $\pi$ except $\pi^i$.
  - $R^i(\pi)$ is the expected payoff of player $i$ if all players follow the strategy profile $\pi$.
  - The set of **best responses** of player $i$ to their opponents' strategies $\pi^{-i}$ is $b^i (\pi^{-i}) = \arg \max _{\pi^i \in \Delta_b^i} R^i (\pi^i, \pi^{-i})$.
  - For $\epsilon > 0, b^i_{\epsilon} (\pi^{-i}) = \left\{ \pi^i \in \Delta_b^i : R^i(\pi^i, \pi^{-i}) \geq R^i(b^i(\pi^{-i}), \pi^{-i}) - \epsilon \right\}$ defines the set of $\epsilon$**-best responses** to the strategy profile $\pi^{-i}$.
  - **A Nash equilibrium** of an extensive-form game is a strategy profile $\pi$ such that $\pi^i \in b^i (\pi^{-i}), \forall i \in \mathcal{N}$. 
  - An $\epsilon$**-Nash equilibrium** is a strategy profile $\pi$ such that $\pi^i \in b^i _{\epsilon}(\pi^{-i}) \forall i \in \mathcal{N}$.  
  


## Normal-Form

- An extensive-form game induces an equivalent **normal-form game** as follows. 
- For each player $i \in \mathcal{N}$ their deterministic strategies, $\Delta_p^i \sub \Delta_b^i$, define a set of normal-form actions, called **pure strategies**. 
- A **mixed strategy** $\Pi^i$ for player $i$ is a probability distribution over their pure strategies. 
- $\Delta^i$ denote the set of all mixed strategies available to player $i$.
- A mixed strategy profile $\Pi \in \times_{i \in \mathcal{N}}\Delta^i$ specifies a mixed strategy for each player
- $R^i : \times _{i \in \mathcal{N}}\Delta^i \rightarrow \mathbb{R}$ determines the expected payoff of player $i$ given a mixed strategy profile.

## Realization-equivalence 

The sequence-form of a game decomposes players' strategies into sequences of actions and probabilities of realizing these sequences. 

- For any player $i \in \mathcal{N}$ of a perfect-recall extensive-form game, each of their information states $u^i \in \mathcal{N}^i$ uniquely defines a **sequence** $\sigma_{u^i}$ of actions
- $\sum^i = \{\sigma_u : u \in \mathcal{U}^i \}$ denote the set of such sequences of player $i$.
- $\sigma_u a$ denote the sequence that extends $\sigma_u$ with action $a$.


**Definition 1**. A **realization plan** of player $i \in \mathcal{N}$ is a function, $x : \sum^i \rightarrow [0, 1]$, such that $x(\emptyset) = 1$ and $\forall \sigma_u \in \mathcal{U}^i : x(\sigma_u) = \sum _{a \in \mathcal{A}(u)} x(\sigma_u a)$.

- A behavioural strategy $\pi$ induces a realization plan $x_\pi(\sigma_u) = \prod_{\left(u^{\prime}, a\right) \in \sigma_{u}} \pi\left(u^{\prime}, a\right)$
- A realization plan induces a behavioural strategy, $\pi(u, a) = \frac{x(\sigma_u a)}{x(\sigma_u)}$

**Definition 2**. Two strategies $\pi_1$ and $\pi_2$ of a player are **realization-equivalent** if for any fixed strategy profile of the other players both strategies, $\pi_1$ and $\pi_2$, define the same probability distribution over the states of the game. 

**Theorem 3** Two strategies are realization-equivalent if and only if they have the same realization plan. 

**Theorem 4** For a player with perfect recall, any mixed strategy is realization-equivalent to a behavioural strategy, and vice versa. 

## Fictitious Play

**Definition 5**. A generalised weakened **fictitious play** is a process of mixed strategies, $\{ \Pi_t \}, \Pi_t \in \times_{i \in \mathcal{N}} \Delta^i$, s.t.

$$
\Pi^i _{t+1} \in (1 - \alpha_{t+1})\Pi^i_t + \alpha_{t+1} (b^i_{\epsilon_t}(\Pi_t^{-t}) + M^i_{t+1} ), \forall i \in \mathcal{N}
$$

with $\alpha_t \rightarrow 0$ and $\epsilon_t \rightarrow 0$ as $t \rightarrow \infty , \sum_{t=1}^\infty \alpha_t = \infty$, $\{M_t\}$ a sequence of perturbations that satisfies $\forall T > 0$

$$
\lim_{t \rightarrow \infty} \sup_k \left\{ \|\sum_{i=t}^{k-1} \alpha_{i+1} M_{i+1} \| s.t. \sum_{i=t}^{k-1} \alpha_{i+1} \leq T \right\} = 0
$$



## Reinforcement Learning

An agent is learning **on-policy** if it gathers these transition tuples by following its own policy. In the **off-policy** setting an agent is learning from experience of another agent or another policy. 



# Extensive-Form Fictitious Play




# References

<a id="1">[1]</a> 
@inproceedings{heinrich2015fictitious,
  title={Fictitious self-play in extensive-form games},
  author={Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle={International conference on machine learning},
  pages={805--813},
  year={2015},
  organization={PMLR}
}


<a id="2">[2]</a> 


