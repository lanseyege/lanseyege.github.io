---
useMath: true
title: 'Notes on "AlphaGo"'
mathjax: true
date: 2021-02-24
permalink: /posts/2021/02/blog-post-40/
tags:
  - AlphaGo
  - RL
  - MCTS
  - policy
---

# AlphaGo


<!-- more -->

AlphaGo is a successful application of (deep) reinforcement learning in board games. 

## Difficulties and Current Methods 

The search space of game Go is extremely huge, it's about $b^d (b \approx 250, d \approx 150)$ possible sequences of moves, where $b$ is the game's breadth (number of legal moves per position) and $d$ is its depth (game length). Exhastive search is infeasible but the effective search space can be reduced by two general principles. 

  - The depth of the search may be reduced by position evaluation 
  - The breadth of the search may be reduced by sampling actions from a policy $p(a | s)$ that is a probability distribution over possible moves $a$ in position $s$. 

Monte Carlo tree search (MCTS) uses Monto Carlo rollouts to estimate the value of each state in a search tree. As more simulations are executed, the search tree grows larger and the relevant values become more accurate. The strongest current (now old) Go progams are based on MCTS. 

## AlphaGo

![AlphaGo training pipeline and Architecture](/images/alphago.png)

AlphaGo use neural networks to reduce the effective depth and breadth of the search tree: evaluating positions using a value network, and sampling actions using a policy network. 

  - AlphaGo begin by training a supervised learning (SL) policy network $p_\sigma$ directly from expert human moves;
  - It also train a fast policy $p_\pi$ that can rapidly sample actions during rollouts;
  - Next, we train a reinforcement learning (RL) policy network $p_\rho$ that improves the SL policy network by optimizing the final outcome of games of self-play;
  - It train a value network $v_\theta$ that predicts the winner of games played by the RL policy network against itself. 

![Monte Carlo Tree Search in AlphaGo](/images/mcts.png)

### Supervised learning of policy networks 

The SL policy network $p_\sigma (a | s)$ alternates between convolutional layers with weights $\sigma$, and rectifier nonlinearities. A final softmax layer outs a probability distribution over all legal moves $a$. The policy network is trained on randomly sampled state-action pairs $(s, a)$, using stocahstic gradient ascent to maximize the likelihood of the human move $a$ selected in state $s$ 

$$
\nabla \sigma \propto \frac{\partial \log p_{\sigma}(a \mid s)}{\partial \sigma}
$$

AlphaGo also trained a faster but less accurate rollout policy $p_\pi (a|s)$, using a linear softmax of small pattern features (see Extended Data Table 4) with weights $\pi$

### Reinforcement Learning of Policy Networks 

This stage aims at improving the policy network by policy gradient reinforcement learning. The RL policy network $p_\rho$ is identical in structure to the SL policy network, and its weights $\rho$ are initialized to the same values, $\rho = \sigma$. AlphaGo play games between the current policy network $p_\rho$ and a randomly selected previous iteration of the policy network. A reward $r(s)$ is used here. It's $0$ for all non-terminal time steps $t < T$. The outcome $z_t = \pm r(S_T)$ is the terminal reward at the game: $+1$ for winning and $-1$ for losing. 

$$
\nabla \rho \propto \frac{\partial \log p_\rho (a_t | s_t)}{\partial \rho} z_t 
$$

### Reinforcement learning of value networks

The final stage of the training pipeline focuses on position evaluation, estimating a value function $v^p (s)$ that predicts the outcome from position $s$ of games played by using policy $p$ for both players 

$$
v^p (s) = \mathbb{E} [z_t | s_t = s, a_{t \ldots T} \sim p]
$$

We train the weights of the value network by regression on state-outcome pairs $(s, z)$, using stochastic gradient descent to minimize the mean squared error (MSE) between the predicted value $v_\theta (s)$, and the corresponding outcome $z$ 

$$
\nabla \theta \propto \frac{\partial v_\theta (s)}{\partial \theta} (z - v_\theta (s))
$$

The naive approach of predicting game outcomes from data consisting of complete games leads to overfitting, for the reason that successive positions are strongly correlated. To mitigate this problem, we generated a new self-play data set. 

### Searching with policy and value networks

AlphaGo combines the policy and value networks in an MCTS algorithm that selects actions by lookahead search. At each time step $t$ of each simulation, an action $a_t$ is selected from state $s_t$

$$
a_t = \arg \max_a (Q(s_t, a) + u (s_t, a))
$$

so as to maximize action value plus a bonus 

$$
u(s, a) \propto \frac{P(s, a)}{ 1 + N(s, a )} 
$$

$P(s, a ) = p_\sigma (a | s)$. A leaf evaluation $V(s_L)$ is 

$$
V(s_L) = (1 - \lambda ) v_\theta (s_L) + \lambda z_L
$$

where the outcome $z_L$ of a random rollout played until terminal step $T$ using the fast rollout policy $p_\pi$. 

At the end of simulation, the action values and visit counts of all traversed edges are updated. Each edge accumulates the visit count and mean evaluation of all simulations passing through that edge

$$
\begin{array}{ll}
N(s , a) = \sum_{i = 1}^n 1(s, a , i) \\
Q(s, a) = \frac{1}{N(s, a)} \sum_{i=1}^n 1(s, a, i) V(s_L^i) \\
\end{array}
$$

where $s_L^i$ is the leaf node from the $i$th simulation, and $1(s, a, i)$ indicates whether an edge $(s, a)$ was traversed during the $i$th simulation.

# References

<a id="1">[1]</a> 
@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}




