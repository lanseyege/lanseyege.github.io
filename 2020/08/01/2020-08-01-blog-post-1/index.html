<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"lanseyege.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Soft Reinforcement Learning: A Shallow Overview">
<meta property="og:type" content="article">
<meta property="og:title" content="Soft Reinforcement Learning: A Shallow Overview ">
<meta property="og:url" content="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Soft Reinforcement Learning: A Shallow Overview">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-31T23:19:37.000Z">
<meta property="article:modified_time" content="2021-02-04T02:05:15.861Z">
<meta property="article:author" content="Ye Yuan">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Soft Reinforcement Learning: A Shallow Overview  | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Soft Reinforcement Learning: A Shallow Overview 
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-01 01:19:37" itemprop="dateCreated datePublished" datetime="2020-08-01T01:19:37+02:00">2020-08-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-04 03:05:15" itemprop="dateModified" datetime="2021-02-04T03:05:15+01:00">2021-02-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Soft-Reinforcement-Learning-A-Shallow-Overview"><a href="#Soft-Reinforcement-Learning-A-Shallow-Overview" class="headerlink" title="Soft Reinforcement Learning: A Shallow Overview"></a>Soft Reinforcement Learning: A Shallow Overview</h1><a id="more"></a>
<h2 id="What-does-“Soft”-mean"><a href="#What-does-“Soft”-mean" class="headerlink" title="What does “Soft” mean?"></a>What does “Soft” mean?</h2><p><strong>Soft Q-learning</strong> <a href="#1">[1]</a> is classical Q-learning with a entropy-regularized item. The “Soft” is relative to conventional approach, which uses a “hard” max in Bellman equation. Comparing to “hard” approach, the “soft” can provide some benefits <a href="#2">[2]</a>:</p>
<ul>
<li>Better Exploration </li>
<li>Fine-tuning Maximum Entropy Policies</li>
<li>Compositionality </li>
<li>Robustness</li>
</ul>
<p><strong>Traditional Bellman Equation</strong><br>In MDPs, the action-value function of a state-action pair $(s,a)$ under policy $\pi$, denoted as $Q_\pi (s, a)$, is the expected return when starting from $s$, following action $a$ and $\pi$ thereafter. The  action-value Bellman equation and Bellman optimality equation are defined as: </p>
<script type="math/tex; mode=display">
\begin{split}
Q(s, a) = \sum_a \pi(a|s) \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q(s^\prime, a)] \\
Q^\star(s, a) = \max_a \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q^\star(s^\prime, a)] \\
\end{split}</script><p>The optimal policy can be derived by:<br>\begin{equation}<br>\pi(s) = \arg \max_a Q^\star (s, a)<br>\end{equation}</p>
<h2 id="Maximum-Entropy-Policies"><a href="#Maximum-Entropy-Policies" class="headerlink" title="Maximum Entropy Policies"></a>Maximum Entropy Policies</h2><p><strong>Entropy and Max Entropy Principle</strong><br>Entropy is an old concept in physicals that is used to describe the randomness in environments. The greater the entropy, the more random the actions the policy gives. The discrete form of entropy is: </p>
<script type="math/tex; mode=display">
\begin{equation}
H(X) = \mathbb{E}_X [I(x)] = - \sum_{x \in X} p(x) \log p(x) 
\end{equation}</script><p>Similarity, the entropy term for policy has this form:</p>
<p>\begin{equation}<br>H(\pi(\cdot | s<em>t)) = - \sum</em>{a\in A} \pi(a|s_t) \log \pi(a|s_t)<br>\end{equation}</p>
<p>The entropy term of policy can help the policy to increase the expoloration ability, by adding more possibilities to some rare actions. That could avoid the agent get stuck into local optimum and attain global optimum, to some extend. The idea of learning such maximum entropy model has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics <a href="#1">[1]</a>. The <strong>principle of maximum entropy</strong> states that the probability distribution with the highest entropy, is the one that best represents the current state of knowledge in the context of precisely stated prior data <a href="#4">[4]</a>. </p>
<p><strong>Stochastic Policy and Maximum Entropy Policies</strong><br>Conventional RL approach is to specify a unimodal policy distribution, centered at the maximal Q-value and extending to the neighbouring actions to provide noise for exploration. Usually, there would be a sampling process which employ a Gaussian distribution, $\pi(a_t|s_t) = N(u(s_t), \sum)$. This would make the policy ignore the action area with low Q-value and thus reduce the exploration ability. Do we have a method to solve this problem? Yes! An obvious solution is to ensure the agent explores all promising states while prioritizing the more promising ones. One way to formalize this idea is to define the policy directly in terms of exponentiated Q-values: $\pi(a\mid s) \propto\exp Q(s, a)$. This density has the form of the Boltzman distribution. What’s more, the policy defined through the energy form is an optimal solution for the maximum-entropy RL objective</p>
<script type="math/tex; mode=display">
\begin{equation}
\pi^\star_\text{MaxEnt} = \arg \max_\pi \mathbb{E}_\pi [\sum_{t=0}^T r_t + H(\pi(\cdot | s_t))]
\end{equation}</script><p>As showed in the paper, the optimal policy for this equation is given by </p>
<script type="math/tex; mode=display">
\begin{equation}
\pi_\text{MaxEnt}^\star (a_t | s_t) = \exp (\frac{1}{\alpha}(Q_\text{soft}^\star(s_t, a_t) - V_\text{soft}^\star(s_t)))
\end{equation}</script><p>where soft Q-function $Q^\star<em>\text{soft}(s_t, a_t)$ and soft value function $V^\star</em>\text{soft}(s_t)$ are defined by:</p>
<script type="math/tex; mode=display">
\begin{split}
Q^\star_\text{soft}(s_t, a_t) &= r_t + \mathbb{E}_{(s_{t+1}, \cdots)\sim \rho_\pi}[\sum_{l=1}^\infty \gamma^l (r_{t+l} + \alpha H(\pi^\star_\text{MaxEnt}(\cdot | s_{t+l})))] \\
V^\star_\text{soft}(s_t) &= \alpha \log \int_{A} \exp(\frac{1}{\alpha}Q^\star_\text{soft}(s_t, a_t) - V^\star_\text{soft}(s_t)) \\
\end{split}</script><p>The idea of learning such maximum entropy models has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics.</p>
<p><strong>Soft Bellman Equation and Soft Q-Learning</strong><br>The soft Bellman equation can be obtained</p>
<script type="math/tex; mode=display">
\begin{equation}
Q(s_t, a_t) = \mathbb{E}[r_t + \gamma \text{softmax}_a Q(s_{t+1}, a)]
\end{equation}</script><p>where $\text{softmax}_a f(a) = \log \int \exp f(a)da$. The soft Bellman equation satisfy contraction property, that means it can convergence to optimal value. We can adopt conventional algorithms for the soft. </p>
<p><strong>Two Challenges in Continuous Domains</strong><br>The first challenge is exact dynamic programming is infeasible, since the soft Bellman equation needs to hold for every state and action, and the softmax involves integrating over the entire action space. The solution is to employ expressive neural network function approximators.  </p>
<p>The second is the optimal policy is defined by an intractable energy-based distribution, which is difficult to sample from. To address this problem, it can employ approximate inference techniques, such as MCMC. TO accelerate inference, the amortized Stein variational gradient descent <a href="#3">[3]</a> can be used to train an inference network to generate approximate samples.  </p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a id="1">[1]</a><br>@article{haarnoja2017reinforcement,<br>  title={Reinforcement learning with deep energy-based policies},<br>  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},<br>  journal={arXiv preprint arXiv:1702.08165},<br>  year={2017}<br>} </p>
<p><a id="2">[2]</a><br><a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/</a></p>
<p><a id="3">[3]</a><br>Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016.</p>
<p><a id="4">[4]</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df">https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df</a></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/posts/2020/08/blog-post-2/" rel="next" title="Soft Reinforcement Learning: More Details">
      Soft Reinforcement Learning: More Details <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-Reinforcement-Learning-A-Shallow-Overview"><span class="nav-number">1.</span> <span class="nav-text">Soft Reinforcement Learning: A Shallow Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#What-does-%E2%80%9CSoft%E2%80%9D-mean"><span class="nav-number">1.1.</span> <span class="nav-text">What does “Soft” mean?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Maximum-Entropy-Policies"><span class="nav-number">1.2.</span> <span class="nav-text">Maximum Entropy Policies</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.3.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

  

</body>
</html>
