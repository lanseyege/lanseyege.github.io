<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"lanseyege.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.1","exturl":false,"sidebar":{"position":"left","display":"always","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Soft Reinforcement Learning: A Shallow Overview">
<meta property="og:type" content="article">
<meta property="og:title" content="Soft Reinforcement Learning: A Shallow Overview ">
<meta property="og:url" content="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/index.html">
<meta property="og:site_name" content="Ye Yuan">
<meta property="og:description" content="Soft Reinforcement Learning: A Shallow Overview">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-07-31T17:19:37.000Z">
<meta property="article:modified_time" content="2021-02-04T09:59:56.627Z">
<meta property="article:author" content="Ye Yuan">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/","path":"2020/08/01/2020-08-01-blog-post-1/","title":"Soft Reinforcement Learning: A Shallow Overview "}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Soft Reinforcement Learning: A Shallow Overview  | Ye Yuan</title>
  






  <script async defer data-website-id="" src=""></script>

  <script defer data-domain="" src=""></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Ye Yuan</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Soft-Reinforcement-Learning-A-Shallow-Overview"><span class="nav-number">1.</span> <span class="nav-text">Soft Reinforcement Learning: A Shallow Overview</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#What-does-%E2%80%9CSoft%E2%80%9D-mean"><span class="nav-number">1.1.</span> <span class="nav-text">What does “Soft” mean?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Maximum-Entropy-Policies"><span class="nav-number">1.2.</span> <span class="nav-text">Maximum Entropy Policies</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">2.</span> <span class="nav-text">References</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Ye Yuan</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">53</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">116</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://lanseyege.github.io/2020/08/01/2020-08-01-blog-post-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Ye Yuan">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ye Yuan">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Soft Reinforcement Learning: A Shallow Overview  | Ye Yuan">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Soft Reinforcement Learning: A Shallow Overview 
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2020-08-01 01:19:37" itemprop="dateCreated datePublished" datetime="2020-08-01T01:19:37+08:00">2020-08-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2021-02-04 17:59:56" itemprop="dateModified" datetime="2021-02-04T17:59:56+08:00">2021-02-04</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Soft-Reinforcement-Learning-A-Shallow-Overview"><a href="#Soft-Reinforcement-Learning-A-Shallow-Overview" class="headerlink" title="Soft Reinforcement Learning: A Shallow Overview"></a>Soft Reinforcement Learning: A Shallow Overview</h1><span id="more"></span>

<h2 id="What-does-“Soft”-mean"><a href="#What-does-“Soft”-mean" class="headerlink" title="What does “Soft” mean?"></a>What does “Soft” mean?</h2><p><strong>Soft Q-learning</strong> <a href="#1">[1]</a> is classical Q-learning with a entropy-regularized item. The “Soft” is relative to conventional approach, which uses a “hard” max in Bellman equation. Comparing to “hard” approach, the “soft” can provide some benefits <a href="#2">[2]</a>:</p>
<ul>
<li>Better Exploration </li>
<li>Fine-tuning Maximum Entropy Policies</li>
<li>Compositionality </li>
<li>Robustness</li>
</ul>
<p><strong>Traditional Bellman Equation</strong><br>In MDPs, the action-value function of a state-action pair $(s,a)$ under policy $\pi$, denoted as $Q_\pi (s, a)$, is the expected return when starting from $s$, following action $a$ and $\pi$ thereafter. The  action-value Bellman equation and Bellman optimality equation are defined as: </p>
<p>$$<br>\begin{split}<br>Q(s, a) &#x3D; \sum_a \pi(a|s) \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q(s^\prime, a)] \<br>Q^\star(s, a) &#x3D; \max_a \sum_{s^\prime,r}\mathbb{T}(s^\prime,r|s,a)[r+\gamma Q^\star(s^\prime, a)] \<br>\end{split}<br>$$</p>
<p>The optimal policy can be derived by:</p>
<p>\begin{equation}<br>\pi(s) &#x3D; \arg \max_a Q^\star (s, a)<br>\end{equation}</p>
<h2 id="Maximum-Entropy-Policies"><a href="#Maximum-Entropy-Policies" class="headerlink" title="Maximum Entropy Policies"></a>Maximum Entropy Policies</h2><p><strong>Entropy and Max Entropy Principle</strong><br>Entropy is an old concept in physicals that is used to describe the randomness in environments. The greater the entropy, the more random the actions the policy gives. The discrete form of entropy is: </p>
<p>$$<br>\begin{equation}<br>H(X) &#x3D; \mathbb{E}<em>X [I(x)] &#x3D; - \sum</em>{x \in X} p(x) \log p(x)<br>\end{equation}<br>$$</p>
<p>Similarity, the entropy term for policy has this form:</p>
<p>\begin{equation}<br>H(\pi(\cdot | s_t)) &#x3D; - \sum_{a\in A} \pi(a|s_t) \log \pi(a|s_t)<br>\end{equation}</p>
<p>The entropy term of policy can help the policy to increase the expoloration ability, by adding more possibilities to some rare actions. That could avoid the agent get stuck into local optimum and attain global optimum, to some extend. The idea of learning such maximum entropy model has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics <a href="#1">[1]</a>. The <strong>principle of maximum entropy</strong> states that the probability distribution with the highest entropy, is the one that best represents the current state of knowledge in the context of precisely stated prior data <a href="#4">[4]</a>. </p>
<p><strong>Stochastic Policy and Maximum Entropy Policies</strong><br>Conventional RL approach is to specify a unimodal policy distribution, centered at the maximal Q-value and extending to the neighbouring actions to provide noise for exploration. Usually, there would be a sampling process which employ a Gaussian distribution, $\pi(a_t|s_t) &#x3D; N(u(s_t), \sum)$. This would make the policy ignore the action area with low Q-value and thus reduce the exploration ability. Do we have a method to solve this problem? Yes! An obvious solution is to ensure the agent explores all promising states while prioritizing the more promising ones. One way to formalize this idea is to define the policy directly in terms of exponentiated Q-values: $\pi(a\mid s) \propto\exp Q(s, a)$. This density has the form of the Boltzman distribution. What’s more, the policy defined through the energy form is an optimal solution for the maximum-entropy RL objective</p>
<p>$$<br>\begin{equation}<br>\pi^\star_\text{MaxEnt} &#x3D; \arg \max_\pi \mathbb{E}<em>\pi [\sum</em>{t&#x3D;0}^T r_t + H(\pi(\cdot | s_t))]<br>\end{equation}<br>$$</p>
<p>As showed in the paper, the optimal policy for this equation is given by </p>
<p>$$<br>\begin{equation}<br>\pi_\text{MaxEnt}^\star (a_t | s_t) &#x3D; \exp (\frac{1}{\alpha}(Q_\text{soft}^\star(s_t, a_t) - V_\text{soft}^\star(s_t)))<br>\end{equation}<br>$$</p>
<p>where soft Q-function $Q^\star_\text{soft}(s_t, a_t)$ and soft value function $V^\star_\text{soft}(s_t)$ are defined by:</p>
<p>$$<br>\begin{split}<br>Q^\star_\text{soft}(s_t, a_t) &amp;&#x3D; r_t + \mathbb{E}<em>{(s</em>{t+1}, \cdots)\sim \rho_\pi}[\sum_{l&#x3D;1}^\infty \gamma^l (r_{t+l} + \alpha H(\pi^\star_\text{MaxEnt}(\cdot | s_{t+l})))] \<br>V^\star_\text{soft}(s_t) &amp;&#x3D; \alpha \log \int_{A} \exp(\frac{1}{\alpha}Q^\star_\text{soft}(s_t, a_t) - V^\star_\text{soft}(s_t)) \<br>\end{split}<br>$$</p>
<p>The idea of learning such maximum entropy models has its origin in statistical modeling, in which the goal is to find the probability distribution that has the highest entropy while still satisfying the observed statistics.</p>
<p><strong>Soft Bellman Equation and Soft Q-Learning</strong><br>The soft Bellman equation can be obtained</p>
<p>$$<br>\begin{equation}<br>Q(s_t, a_t) &#x3D; \mathbb{E}[r_t + \gamma \text{softmax}<em>a Q(s</em>{t+1}, a)]<br>\end{equation}<br>$$</p>
<p>where $\text{softmax}_a f(a) &#x3D; \log \int \exp f(a)da$. The soft Bellman equation satisfy contraction property, that means it can convergence to optimal value. We can adopt conventional algorithms for the soft. </p>
<p><strong>Two Challenges in Continuous Domains</strong><br>The first challenge is exact dynamic programming is infeasible, since the soft Bellman equation needs to hold for every state and action, and the softmax involves integrating over the entire action space. The solution is to employ expressive neural network function approximators.  </p>
<p>The second is the optimal policy is defined by an intractable energy-based distribution, which is difficult to sample from. To address this problem, it can employ approximate inference techniques, such as MCMC. TO accelerate inference, the amortized Stein variational gradient descent <a href="#3">[3]</a> can be used to train an inference network to generate approximate samples.  </p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a id="1">[1]</a><br>@article{haarnoja2017reinforcement,<br>  title&#x3D;{Reinforcement learning with deep energy-based policies},<br>  author&#x3D;{Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},<br>  journal&#x3D;{arXiv preprint arXiv:1702.08165},<br>  year&#x3D;{2017}<br>} </p>
<p><a id="2">[2]</a><br><a target="_blank" rel="noopener" href="https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">https://bair.berkeley.edu/blog/2017/10/06/soft-q-learning/</a></p>
<p><a id="3">[3]</a><br>Liu, Q. and Wang, D. Stein variational gradient descent: A general purpose bayesian inference algorithm. In Advances In Neural Information Processing Systems, pp. 2370–2378, 2016.</p>
<p><a id="4">[4]</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df">https://towardsdatascience.com/entropy-regularization-in-reinforcement-learning-a6fa6d7598df</a></p>

    </div>

    
    
    

    <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-item">
            </div>
            <div class="post-nav-item">
                <a href="/posts/2020/08/blog-post-2/" rel="next" title="Soft Reinforcement Learning: More Details">
                  Soft Reinforcement Learning: More Details <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Ye Yuan</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
